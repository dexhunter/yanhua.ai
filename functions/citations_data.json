{
  "last_updated": "2025-07-17 00:12:03 UTC",
  "target_paper": "https://www.arxiv.org/abs/2502.13138",
  "arxiv_id": "2502.13138",
  "total_citations": 32,
  "h_index": 2,
  "recent_citations": 12,
  "avg_citations_per_month": "4.57",
  "timeline": [
    {
      "date": "2025-01-01",
      "citations": 3
    },
    {
      "date": "2025-03-01",
      "citations": 4
    },
    {
      "date": "2025-04-01",
      "citations": 5
    },
    {
      "date": "2025-05-01",
      "citations": 12
    },
    {
      "date": "2025-06-01",
      "citations": 20
    },
    {
      "date": "2025-07-01",
      "citations": 32
    }
  ],
  "papers": [
    {
      "title": "MMOne: Representing Multiple Modalities in One Scene",
      "authors": "Zhifeng Gu, Bing Wang",
      "journal": "arXiv",
      "snippet": "Humans perceive the world through multimodal cues to understand and interact\nwith the environment. Learning a scene representation for multiple modalities\nenhances comprehension of the physical world. However, modality conflicts,\narising from inherent distinctions among different modalities, present two\ncritical challenges: property disparity and granularity disparity. To address\nthese challenges, we propose a general framework, MMOne, to represent multiple\nmodalities in one scene, which can be readily extended to additional\nmodalities. Specifically, a modality modeling module with a novel modality\nindicator is proposed to capture the unique properties of each modality.\nAdditionally, we design a multimodal decomposition mechanism to separate\nmulti-modal Gaussians into single-modal Gaussians based on modality\ndifferences. We address the essential distinctions among modalities by\ndisentangling multimodal information into shared and modality-specific\ncomponents, resulting in a more compact and efficient multimodal scene\nrepresentation. Extensive experiments demonstrate that our method consistently\nenhances the representation capability for each modality and is scalable to\nadditional modalities. The code is available at\nhttps://github.com/Neal2020GitHub/MMOne.",
      "link": "http://arxiv.org/abs/2507.11129v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Relative Entropy Pathwise Policy Optimization",
      "authors": "Claas Voelcker, Axel Brunnbauer, Marcel Hussing, Michal Nauman, Pieter Abbeel, Eric Eaton, Radu Grosu, Amir-massoud Farahmand, Igor Gilitschenski",
      "journal": "arXiv",
      "snippet": "Score-function policy gradients have delivered strong results in\ngame-playing, robotics and language-model fine-tuning. Yet its high-variance\noften undermines training stability. On the other hand, pathwise policy\ngradients alleviate the training variance, but are reliable only when driven by\nan accurate action-conditioned value function which is notoriously hard to\ntrain without relying on past off-policy data. In this paper, we discuss how to\nconstruct a value-gradient driven, on-policy algorithm that allow training\nQ-value models purely from on-policy data, unlocking the possibility of using\npathwise policy updates in the context of on-policy learning. We show how to\nbalance stochastic policies for exploration with constrained policy updates for\nstable training, and evaluate important architectural components that\nfacilitate accurate value function learning. Building on these insights, we\npropose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient\non-policy algorithm that combines the sample-efficiency of pathwise policy\ngradients with the simplicity and minimal memory footprint of standard\non-policy learning. We demonstrate that REPPO provides strong empirical\nperformance at decreased sample requirements, wall-clock time, memory footprint\nas well as high hyperparameter robustness in a set of experiments on two\nstandard GPU-parallelized benchmarks.",
      "link": "http://arxiv.org/abs/2507.11019v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Mixture of Experts in Large Language Models",
      "authors": "Danyang Zhang, Junhao Song, Ziqian Bi, Yingfang Yuan, Tianyang Wang, Joe Yeong, Junfeng Hao",
      "journal": "arXiv",
      "snippet": "This paper presents a comprehensive review of the Mixture-of-Experts (MoE)\narchitecture in large language models, highlighting its ability to\nsignificantly enhance model performance while maintaining minimal computational\noverhead. Through a systematic analysis spanning theoretical foundations, core\narchitectural designs, and large language model (LLM) applications, we examine\nexpert gating and routing mechanisms, hierarchical and sparse MoE\nconfigurations, meta-learning approaches, multimodal and multitask learning\nscenarios, real-world deployment cases, and recent advances and challenges in\ndeep learning. Our analysis identifies key advantages of MoE, including\nsuperior model capacity compared to equivalent Bayesian approaches, improved\ntask-specific performance, and the ability to scale model capacity efficiently.\nWe also underscore the importance of ensuring expert diversity, accurate\ncalibration, and reliable inference aggregation, as these are essential for\nmaximizing the effectiveness of MoE architectures. Finally, this review\noutlines current research limitations, open challenges, and promising future\ndirections, providing a foundation for continued innovation in MoE architecture\nand its applications.",
      "link": "http://arxiv.org/abs/2507.11181v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "U-RWKV: Lightweight medical image segmentation with direction-adaptive\n  RWKV",
      "authors": "Hongbo Ye, Fenghe Tang, Peiang Zhao, Zhen Huang, Dexin Zhao, Minghao Bian, S. Kevin Zhou",
      "journal": "arXiv",
      "snippet": "Achieving equity in healthcare accessibility requires lightweight yet\nhigh-performance solutions for medical image segmentation, particularly in\nresource-limited settings. Existing methods like U-Net and its variants often\nsuffer from limited global Effective Receptive Fields (ERFs), hindering their\nability to capture long-range dependencies. To address this, we propose U-RWKV,\na novel framework leveraging the Recurrent Weighted Key-Value(RWKV)\narchitecture, which achieves efficient long-range modeling at O(N)\ncomputational cost. The framework introduces two key innovations: the\nDirection-Adaptive RWKV Module(DARM) and the Stage-Adaptive\nSqueeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan\nmechanisms to aggregate contextual cues across images, mitigating directional\nbias while preserving global context and maintaining high computational\nefficiency. SASE dynamically adapts its architecture to different feature\nextraction stages, balancing high-resolution detail preservation and semantic\nrelationship capture. Experiments demonstrate that U-RWKV achieves\nstate-of-the-art segmentation performance with high computational efficiency,\noffering a practical solution for democratizing advanced medical imaging\ntechnologies in resource-constrained environments. The code is available at\nhttps://github.com/hbyecoding/U-RWKV.",
      "link": "http://arxiv.org/abs/2507.11415v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical\n  Reasoning?",
      "authors": "Soumadeep Saha, Akshay Chaturvedi, Saptarshi Saha, Utpal Garain, Nicholas Asher",
      "journal": "arXiv",
      "snippet": "Chain-of-thought traces have been shown to improve performance of large\nlanguage models in a plethora of reasoning tasks, yet there is no consensus on\nthe mechanism through which this performance boost is achieved. To shed more\nlight on this, we introduce Causal CoT Graphs (CCGs), which are directed\nacyclic graphs automatically extracted from reasoning traces that model\nfine-grained causal dependencies in the language model output. A collection of\n$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their\nassociated CCGs are compiled into our dataset -- \\textbf{KisMATH}. Our detailed\nempirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in\nthe CCG are mediators for the final answer, a condition necessary for\nreasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating\nthat models internally realise structures akin to our graphs. KisMATH enables\ncontrolled, graph-aligned interventions and opens up avenues for further\ninvestigation into the role of chain-of-thought in LLM reasoning.",
      "link": "http://arxiv.org/abs/2507.11408v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Streaming 4D Visual Geometry Transformer",
      "authors": "Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, Jiwen Lu",
      "journal": "arXiv",
      "snippet": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
      "link": "http://arxiv.org/abs/2507.11539v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep\n  Learning Compiler Techniques",
      "authors": "Yiqi Liu, Yuqi Xue, Noelle Crawford, Jilong Xue, Jian Huang",
      "journal": "arXiv",
      "snippet": "To meet the increasing demand of deep learning (DL) models, AI chips are\nemploying both off-chip memory (e.g., HBM) and high-bandwidth low-latency\ninterconnect for direct inter-core data exchange. However, it is not easy to\nexplore the efficiency of these inter-core connected AI (ICCA) chips, due to a\nfundamental tussle among compute (per-core execution), communication\n(inter-core data exchange), and I/O (off-chip data access).\n  In this paper, we develop Elk, a DL compiler framework to maximize the\nefficiency of ICCA chips by jointly trading off all the three performance\nfactors discussed above. Elk structures these performance factors into\nconfigurable parameters and forms a global trade-off space in the DL compiler.\nTo systematically explore this space and maximize overall efficiency, Elk\nemploys a new inductive operator scheduling policy and a cost-aware on-chip\nmemory allocation algorithm. It generates globally optimized execution plans\nthat best overlap off-chip data loading and on-chip execution. To examine the\nefficiency of Elk, we build a full-fledged emulator based on a real ICCA chip\nIPU-POD4, and an ICCA chip simulator for sensitivity analysis with different\ninterconnect network topologies. Elk achieves 94% of the ideal roofline\nperformance of ICCA chips on average, showing the benefits of supporting large\nDL models on ICCA chips. We also show Elk's capability of enabling architecture\ndesign space exploration for new ICCA chip development.",
      "link": "http://arxiv.org/abs/2507.11506v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Illuminating the Three Dogmas of Reinforcement Learning under\n  Evolutionary Light",
      "authors": "Mani Hamidi, Terrence W. Deacon",
      "journal": "arXiv",
      "snippet": "Three core tenets of reinforcement learning (RL)--concerning the definition\nof agency, the objective of learning, and the scope of the reward\nhypothesis--have been highlighted as key targets for conceptual revision, with\nmajor implications for theory and application. We propose a framework, inspired\nby open-ended evolutionary theory, to reconsider these three \"dogmas.\" We\nrevisit each assumption and address related concerns raised alongside them. To\nmake our arguments relevant to RL as a model of biological learning, we first\nestablish that evolutionary dynamics can plausibly operate within living brains\nover an individual's lifetime, and are not confined to cross-generational\nprocesses. We begin by revisiting the second dogma, drawing on evolutionary\ninsights to enrich the \"adaptation-rather-than-search\" view of learning. We\nthen address the third dogma regarding the limits of the reward hypothesis,\nusing analogies from evolutionary fitness to illuminate the scalar reward vs.\nmulti-objective debate. After discussing practical implications for exploration\nin RL, we turn to the first--and arguably most fundamental--issue: the absence\nof a formal account of agency. We argue that unlike the other two problems, the\nevolutionary paradigm alone cannot resolve the agency question, though it\ngestures in a productive direction. We advocate integrating ideas from\norigins-of-life theory, where the thermodynamics of sustenance and replication\noffer promising foundations for understanding agency and resource-constrained\nreinforcement learning in biological systems.",
      "link": "http://arxiv.org/abs/2507.11482v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Personalized Exercise Recommendation with Semantically-Grounded\n  Knowledge Tracing",
      "authors": "Yilmazcan Ozyurt, Tunaberk Almaci, Stefan Feuerriegel, Mrinmaya Sachan",
      "journal": "arXiv",
      "snippet": "We introduce ExRec, a general framework for personalized exercise\nrecommendation with semantically-grounded knowledge tracing. Our method builds\non the observation that existing exercise recommendation approaches simulate\nstudent performance via knowledge tracing (KT) but they often overlook two key\naspects: (a) the semantic content of questions and (b) the sequential,\nstructured progression of student learning. To address this, our ExRec presents\nan end-to-end pipeline, from annotating the KCs of questions and learning their\nsemantic representations to training KT models and optimizing several\nreinforcement learning (RL) methods. Moreover, we improve standard\nQ-learning-based continuous RL methods via a tailored model-based value\nestimation (MVE) approach that directly leverages the components of KT model in\nestimating cumulative knowledge improvement. We validate the effectiveness of\nour ExRec using various RL methods across four real-world tasks with different\neducational goals in online math learning. We further show that ExRec\ngeneralizes robustly to new, unseen questions and that it produces\ninterpretable student learning trajectories. Together, our findings highlight\nthe promise of KT-guided RL for effective personalization in education.",
      "link": "http://arxiv.org/abs/2507.11060v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for\n  Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging",
      "authors": "Arefin Ittesafun Abian, Ripon Kumar Debnath, Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Md Rafiqul Islam, Asif Karim, Reem E. Mohamed, Sami Azam",
      "journal": "arXiv",
      "snippet": "Accurate liver and tumor segmentation on abdominal CT images is critical for\nreliable diagnosis and treatment planning, but remains challenging due to\ncomplex anatomical structures, variability in tumor appearance, and limited\nannotated data. To address these issues, we introduce Hyperbolic-convolutions\nAdaptive-temporal-attention with Neural-representation and Synaptic-plasticity\nNetwork (HANS-Net), a novel segmentation framework that synergistically\ncombines hyperbolic convolutions for hierarchical geometric representation, a\nwavelet-inspired decomposition module for multi-scale texture learning, a\nbiologically motivated synaptic plasticity mechanism for adaptive feature\nenhancement, and an implicit neural representation branch to model fine-grained\nand continuous anatomical boundaries. Additionally, we incorporate\nuncertainty-aware Monte Carlo dropout to quantify prediction confidence and\nlightweight temporal attention to improve inter-slice consistency without\nsacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate\nthat HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an\naverage symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap\nerror (VOE) of 11.91%. Furthermore, cross-dataset validation on the\n3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of\n1.525 mm, and VOE of 19.71%, indicating strong generalization across different\ndatasets. These results confirm the effectiveness and robustness of HANS-Net in\nproviding anatomically consistent, accurate, and confident liver and tumor\nsegmentation.",
      "link": "http://arxiv.org/abs/2507.11325v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Quantitative multi-metabolite imaging of Parkinson's disease using AI\n  boosted molecular MRI",
      "authors": "Hagar Shmuely, Michal Rivlin, Or Perlman",
      "journal": "arXiv",
      "snippet": "Traditional approaches for molecular imaging of Parkinson's disease (PD) in\nvivo require radioactive isotopes, lengthy scan times, or deliver only low\nspatial resolution. Recent advances in saturation transfer-based PD magnetic\nresonance imaging (MRI) have provided biochemical insights, although the image\ncontrast is semi-quantitative and nonspecific. Here, we combined a rapid\nmolecular MRI acquisition paradigm with deep learning based reconstruction for\nmulti-metabolite quantification of glutamate, mobile proteins, semisolid, and\nmobile macromolecules in an acute MPTP\n(1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine) mouse model. The quantitative\nparameter maps are in general agreement with the histology and MR spectroscopy,\nand demonstrate that semisolid magnetization transfer (MT), amide, and\naliphatic relayed nuclear Overhauser effect (rNOE) proton volume fractions may\nserve as PD biomarkers.",
      "link": "http://arxiv.org/abs/2507.11329v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language\n  Models on Software Engineering Tasks",
      "authors": "Pavel Adamenko, Mikhail Ivanov, Aidar Valeev, Rodion Levichev, Pavel Zadorozhny, Ivan Lopatin, Dmitry Babayev, Alena Fenogenova, Valentin Malykh",
      "journal": "arXiv",
      "snippet": "The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08\\% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025.",
      "link": "http://arxiv.org/abs/2507.11059v1",
      "published_date": "July 15, 2025",
      "published_date_sort": "2025-07-15",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement",
      "authors": "J Nam, J Yoon, J Chen, J Shin, SÖ Arık… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Agents based on large language models (LLMs) for machine learning engineering (MLE) can automatically implement ML models via code generation. However, existing approaches …",
      "link": "https://arxiv.org/abs/2506.15692",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science",
      "authors": "A Luo, X Xian, J Du, F Tian, G Wang, M Zhong… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as …",
      "link": "https://arxiv.org/abs/2506.13992",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "authors": "Y Ou, Y Luo, J Zheng, L Wei, S Qiao, J Zhang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire …",
      "link": "https://arxiv.org/abs/2506.10974",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Towards Community-Driven Agents for Machine Learning Engineering",
      "authors": "S Li, W Sun, S Li, A Talwalkar, Y Yang - arXiv preprint arXiv:2506.20640, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given …",
      "link": "https://arxiv.org/abs/2506.20640",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AI Scientists Fail Without Strong Implementation Capability",
      "authors": "M Zhu, Q Xie, Y Weng, J Wu, Z Lin, L Yang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the …",
      "link": "https://arxiv.org/abs/2506.01372",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications",
      "authors": "R Xu, J Peng - arXiv preprint arXiv:2506.12594, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "This survey examines the rapidly evolving field of Deep Research systems--AI-powered applications that automate complex research workflows through the integration of large …",
      "link": "https://arxiv.org/abs/2506.12594",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
      "authors": "B Zhao, D Magka, M Jiang, X Li, R Raileanu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce …",
      "link": "https://arxiv.org/abs/2506.22419",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research",
      "authors": "S Yan, R Li, Z Luo, Z Wang, D Li, L Jing, K He… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of …",
      "link": "https://arxiv.org/abs/2506.17335",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis",
      "authors": "H Li, H Liu, T Zhu, T Guo, Z Zheng, X Deng… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with …",
      "link": "https://arxiv.org/abs/2505.18223",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents",
      "authors": "Y Cai, X Li, M Goswami, M Wiliński, G Welter… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating Artificial Intelligence (AI) agents on time series machine learning engineering challenges. Existing …",
      "link": "https://arxiv.org/abs/2505.13291",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Mlzero: A multi-agent system for end-to-end machine learning automation",
      "authors": "H Fang, B Han, N Erickson, X Zhang, S Zhou… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly …",
      "link": "https://arxiv.org/abs/2505.13941",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving",
      "authors": "H Wang, Z Ni, S Zhang, S Lu, S Hu, Z He, C Hu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world …",
      "link": "https://arxiv.org/abs/2505.21577",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "From automation to autonomy: A survey on large language models in scientific discovery",
      "authors": "T Zheng, Z Deng, HT Tsang, W Wang, J Bai… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and …",
      "link": "https://arxiv.org/abs/2505.13259",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 2,
      "institutions": "Unknown"
    },
    {
      "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
      "authors": "S Li, T Marwah, J Shen, W Sun, A Risteski… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert …",
      "link": "https://arxiv.org/abs/2505.08783",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Mle-dojo: Interactive environments for empowering llm agents in machine learning engineering",
      "authors": "R Qiang, Y Zhuang, Y Li, R Zhang, C Li… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative …",
      "link": "https://arxiv.org/abs/2505.07782",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Co-bench: Benchmarking language model agents in algorithm search for combinatorial optimization",
      "authors": "W Sun, S Feng, S Li, Y Yang - arXiv preprint arXiv:2504.04310, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial …",
      "link": "https://arxiv.org/abs/2504.04310",
      "published_date": "April 2025",
      "published_date_sort": "2025-04-01",
      "citations": 6,
      "institutions": "Unknown"
    },
    {
      "title": "Measuring ai ability to complete long tasks",
      "authors": "T Kwa, B West, J Becker, A Deng, K Garcia… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human …",
      "link": "https://arxiv.org/abs/2503.14499",
      "published_date": "March 2025",
      "published_date_sort": "2025-03-01",
      "citations": 20,
      "institutions": "Unknown"
    },
    {
      "title": "PiML: Automated Machine Learning Workflow Optimization using LLM Agents",
      "authors": "A Chopde, F Pettiwala, S Kirubananth, SK Botla… - AutoML 2025 Methods … - openreview.net",
      "journal": "Google Scholar Result",
      "snippet": "In this paper, we introduce PiML-Persistent Iterative Machine Learning agentic framework, a novel automated pipeline specifically designed for solving real-world machine learning (ML) …",
      "link": "https://openreview.net/forum?id=Nw1qBpsjZz",
      "published_date": "2025",
      "published_date_sort": "2025-01-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AutoCluster: Un agent pour le clustering basé sur les grands modèles de langue",
      "authors": "E Versmée, Y Remil, M Kaytoue, J Velcin - coria-taln-2025.lis-lab.fr",
      "journal": "Google Scholar Result",
      "snippet": "Cette recherche présente AutoCluster, un agent basé sur les grands modèles de langue pour des tâches de classification non supervisée. Nous concevons trois agents dont deux …",
      "link": "https://coria-taln-2025.lis-lab.fr/wp-content/uploads/2025/06/CORIA-TALN_2025_paper_124.pdf",
      "published_date": "2025",
      "published_date_sort": "2025-01-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Towards Pragmatic Time Series Intelligence",
      "authors": "M Goswami - 2025 - search.proquest.com",
      "journal": "Google Scholar Result",
      "snippet": "This thesis aims to democratize time series intelligence by making advanced modeling capabilities accessible to users without specialized machine learning knowledge. We …",
      "link": "https://search.proquest.com/openview/24ff18df8583af7a835b667b59b216b9/1?pq-origsite=gscholar&cbl=18750&diss=y",
      "published_date": "2025",
      "published_date_sort": "2025-01-01",
      "citations": 0,
      "institutions": "Unknown"
    }
  ]
}