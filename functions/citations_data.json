{
  "last_updated": "2025-09-19 00:10:33 UTC",
  "target_paper": "https://www.arxiv.org/abs/2502.13138",
  "arxiv_id": "2502.13138",
  "total_citations": 34,
  "h_index": 5,
  "recent_citations": 16,
  "avg_citations_per_month": "4.86",
  "timeline": [
    {
      "date": "2025-03-01",
      "citations": 1
    },
    {
      "date": "2025-04-01",
      "citations": 2
    },
    {
      "date": "2025-05-01",
      "citations": 8
    },
    {
      "date": "2025-06-01",
      "citations": 15
    },
    {
      "date": "2025-07-01",
      "citations": 17
    },
    {
      "date": "2025-08-01",
      "citations": 18
    },
    {
      "date": "2025-09-01",
      "citations": 34
    }
  ],
  "papers": [
    {
      "title": "Breaking the Cycle of Incarceration With Targeted Mental Health\n  Outreach: A Case Study in Machine Learning for Public Policy",
      "authors": "Kit T. Rodolfa, Erika Salomon, Jin Yao, Steve Yoder, Robert Sullivan, Kevin McGuire, Allie Dickinson, Rob MacDougall, Brian Seidler, Christina Sung, Claire Herdeman, Rayid Ghani",
      "journal": "arXiv",
      "snippet": "Many incarcerated individuals face significant and complex challenges,\nincluding mental illness, substance dependence, and homelessness, yet jails and\nprisons are often poorly equipped to address these needs. With little support\nfrom the existing criminal justice system, these needs can remain untreated and\nworsen, often leading to further offenses and a cycle of incarceration with\nadverse outcomes both for the individual and for public safety, with\nparticularly large impacts on communities of color that continue to widen the\nalready extensive racial disparities in criminal justice outcomes. Responding\nto these failures, a growing number of criminal justice stakeholders are\nseeking to break this cycle through innovative approaches such as\ncommunity-driven and alternative approaches to policing, mentoring, community\nbuilding, restorative justice, pretrial diversion, holistic defense, and social\nservice connections. Here we report on a collaboration between Johnson County,\nKansas, and Carnegie Mellon University to perform targeted, proactive mental\nhealth outreach in an effort to reduce reincarceration rates.\n  This paper describes the data used, our predictive modeling approach and\nresults, as well as the design and analysis of a field trial conducted to\nconfirm our model's predictive power, evaluate the impact of this targeted\noutreach, and understand at what level of reincarceration risk outreach might\nbe most effective. Through this trial, we find that our model is highly\npredictive of new jail bookings, with more than half of individuals in the\ntrial's highest-risk group returning to jail in the following year. Outreach\nwas most effective among these highest-risk individuals, with impacts on mental\nhealth utilization, EMS dispatches, and criminal justice involvement.",
      "link": "http://arxiv.org/abs/2509.14129v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "A Variational Framework for Residual-Based Adaptivity in Neural PDE\n  Solvers and Operator Learning",
      "authors": "Juan Diego Toscano, Daniel T. Chen, Vivek Oommen, George Em Karniadakis",
      "journal": "arXiv",
      "snippet": "Residual-based adaptive strategies are widely used in scientific machine\nlearning but remain largely heuristic. We introduce a unifying variational\nframework that formalizes these methods by integrating convex transformations\nof the residual. Different transformations correspond to distinct objective\nfunctionals: exponential weights target the minimization of uniform error,\nwhile linear weights recover the minimization of quadratic error. Within this\nperspective, adaptive weighting is equivalent to selecting sampling\ndistributions that optimize the primal objective, thereby linking\ndiscretization choices directly to error metrics. This principled approach\nyields three benefits: (1) it enables systematic design of adaptive schemes\nacross norms, (2) reduces discretization error through variance reduction of\nthe loss estimator, and (3) enhances learning dynamics by improving the\ngradient signal-to-noise ratio. Extending the framework to operator learning,\nwe demonstrate substantial performance gains across optimizers and\narchitectures. Our results provide a theoretical justification of\nresidual-based adaptivity and establish a foundation for principled\ndiscretization and training strategies.",
      "link": "http://arxiv.org/abs/2509.14198v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Data Denoising and Derivative Estimation for Data-Driven Modeling of\n  Nonlinear Dynamical Systems",
      "authors": "Jiaqi Yao, Lewis Mitchell, John Maclean, Hemanth Saratchandran",
      "journal": "arXiv",
      "snippet": "Data-driven modeling of nonlinear dynamical systems is often hampered by\nmeasurement noise. We propose a denoising framework, called Runge-Kutta and\nTotal Variation Based Implicit Neural Representation (RKTV-INR), that\nrepresents the state trajectory with an implicit neural representation (INR)\nfitted directly to noisy observations. Runge-Kutta integration and total\nvariation are imposed as constraints to ensure that the reconstructed state is\na trajectory of a dynamical system that remains close to the original data. The\ntrained INR yields a clean, continuous trajectory and provides accurate\nfirst-order derivatives via automatic differentiation. These denoised states\nand derivatives are then supplied to Sparse Identification of Nonlinear\nDynamics (SINDy) to recover the governing equations. Experiments demonstrate\neffective noise suppression, precise derivative estimation, and reliable system\nidentification.",
      "link": "http://arxiv.org/abs/2509.14219v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
      "authors": "Jiayi Pan, Jiaming Xu, Yongkang Zhou, Guohao Dai",
      "journal": "arXiv",
      "snippet": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
      "link": "http://arxiv.org/abs/2509.13848v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Learning Minimal Representations of Many-Body Physics from Snapshots of\n  a Quantum Simulator",
      "authors": "Frederik Møller, Gabriel Fernández-Fernández, Thomas Schweigler, Paulin de Schoulepnikoff, Jörg Schmiedmayer, Gorka Muñoz-Gil",
      "journal": "arXiv",
      "snippet": "Analog quantum simulators provide access to many-body dynamics beyond the\nreach of classical computation. However, extracting physical insights from\nexperimental data is often hindered by measurement noise, limited observables,\nand incomplete knowledge of the underlying microscopic model. Here, we develop\na machine learning approach based on a variational autoencoder (VAE) to analyze\ninterference measurements of tunnel-coupled one-dimensional Bose gases, which\nrealize the sine-Gordon quantum field theory. Trained in an unsupervised\nmanner, the VAE learns a minimal latent representation that strongly correlates\nwith the equilibrium control parameter of the system. Applied to\nnon-equilibrium protocols, the latent space uncovers signatures of frozen-in\nsolitons following rapid cooling, and reveals anomalous post-quench dynamics\nnot captured by conventional correlation-based methods. These results\ndemonstrate that generative models can extract physically interpretable\nvariables directly from noisy and sparse experimental data, providing\ncomplementary probes of equilibrium and non-equilibrium physics in quantum\nsimulators. More broadly, our work highlights how machine learning can\nsupplement established field-theoretical techniques, paving the way for\nscalable, data-driven discovery in quantum many-body systems.",
      "link": "http://arxiv.org/abs/2509.13821v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "An Empirical Study on Failures in Automated Issue Solving",
      "authors": "Simiao Liu, Fang Liu, Liehao Li, Xin Tan, Yinghao Zhu, Xiaoli Lian, Li Zhang",
      "journal": "arXiv",
      "snippet": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign.",
      "link": "http://arxiv.org/abs/2509.13941v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "NIRVANA: Structured pruning reimagined for large language models\n  compression",
      "authors": "Mengting Ai, Tianxin Wei, Sirui Chen, Jingrui He",
      "journal": "arXiv",
      "snippet": "Structured pruning of large language models (LLMs) offers substantial\nefficiency improvements by removing entire hidden units, yet current approaches\noften suffer from significant performance degradation, particularly in\nzero-shot settings, and necessitate costly recovery techniques such as\nsupervised fine-tuning (SFT) or adapter insertion. To address these critical\nshortcomings, we introduce NIRVANA, a novel pruning method explicitly designed\nto balance immediate zero-shot accuracy preservation with robust fine-tuning\ncapability. Leveraging a first-order saliency criterion derived from the Neural\nTangent Kernel under Adam optimization dynamics, NIRVANA provides a\ntheoretically grounded pruning strategy that respects essential model training\nbehaviors. To further address the unique challenges posed by structured\npruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across\nlayers and modules (attention vs. MLP), which adjusts pruning intensity between\nmodules in a globally balanced manner. Additionally, to mitigate the high\nsensitivity of pruning decisions to calibration data quality, we propose a\nsimple yet effective KL divergence-based calibration data selection strategy,\nensuring more reliable and task-agnostic pruning outcomes. Comprehensive\nexperiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA\noutperforms existing structured pruning methods under equivalent sparsity\nconstraints, providing a theoretically sound and practical approach to LLM\ncompression. The code is available at\nhttps://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.",
      "link": "http://arxiv.org/abs/2509.14230v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Large Language Model-Empowered Decision Transformer for UAV-Enabled Data\n  Collection",
      "authors": "Zhixion Chen, Jiangzhou Wang, and Hyundong Shin, Arumugam Nallanathan",
      "journal": "arXiv",
      "snippet": "The deployment of unmanned aerial vehicles (UAVs) for reliable and\nenergy-efficient data collection from spatially distributed devices holds great\npromise in supporting diverse Internet of Things (IoT) applications.\nNevertheless, the limited endurance and communication range of UAVs necessitate\nintelligent trajectory planning. While reinforcement learning (RL) has been\nextensively explored for UAV trajectory optimization, its interactive nature\nentails high costs and risks in real-world environments. Offline RL mitigates\nthese issues but remains susceptible to unstable training and heavily rely on\nexpert-quality datasets. To address these challenges, we formulate a joint UAV\ntrajectory planning and resource allocation problem to maximize energy\nefficiency of data collection. The resource allocation subproblem is first\ntransformed into an equivalent linear programming formulation and solved\noptimally with polynomial-time complexity. Then, we propose a large language\nmodel (LLM)-empowered critic-regularized decision transformer (DT) framework,\ntermed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we\nincorporate critic networks to regularize the DT model training, thereby\nintegrating the sequence modeling capabilities of DT with critic-based value\nguidance to enable learning effective policies from suboptimal datasets.\nFurthermore, to mitigate the data-hungry nature of transformer models, we\nemploy a pre-trained LLM as the transformer backbone of the DT model and adopt\na parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid\nadaptation to UAV control tasks with small-scale dataset and low computational\noverhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark\nonline and offline RL methods, achieving up to 36.7\\% higher energy efficiency\nthan the current state-of-the-art DT approaches.",
      "link": "http://arxiv.org/abs/2509.13934v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency",
      "authors": "Colin Hong, Xu Guo, Anand Chaanan Singh, Esha Choukse, Dmitrii Ustiugov",
      "journal": "arXiv",
      "snippet": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.",
      "link": "http://arxiv.org/abs/2509.13990v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and\n  Sizing for AMS Circuits",
      "authors": "Ziming Wei, Zichen Kong, Yuan Wang, David Z. Pan, Xiyuan Tang",
      "journal": "arXiv",
      "snippet": "Analog and mixed-signal circuit design remains challenging due to the\nshortage of high-quality data and the difficulty of embedding domain knowledge\ninto automated flows. Traditional black-box optimization achieves sampling\nefficiency but lacks circuit understanding, which often causes evaluations to\nbe wasted in low-value regions of the design space. In contrast, learning-based\nmethods embed structural knowledge but are case-specific and costly to retrain.\nRecent attempts with large language models show potential, yet they often rely\non manual intervention, limiting generality and transparency. We propose\nTopoSizing, an end-to-end framework that performs robust circuit understanding\ndirectly from raw netlists and translates this knowledge into optimization\ngains. Our approach first applies graph algorithms to organize circuits into a\nhierarchical device-module-stage representation. LLM agents then execute an\niterative hypothesis-verification-refinement loop with built-in consistency\nchecks, producing explicit annotations. Verified insights are integrated into\nBayesian optimization through LLM-guided initial sampling and\nstagnation-triggered trust-region updates, improving efficiency while\npreserving feasibility.",
      "link": "http://arxiv.org/abs/2509.14169v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "An Exhaustive DPLL Approach to Model Counting over Integer Linear\n  Constraints with Simplification Techniques",
      "authors": "Mingwei Zhang, Zhenhao Gu, Liangda Fang, Cunjing Ge, Ziliang Chen, Zhao-Rong Lai, Quanlong Guan",
      "journal": "arXiv",
      "snippet": "Linear constraints are one of the most fundamental constraints in fields such\nas computer science, operations research and optimization. Many applications\nreduce to the task of model counting over integer linear constraints (MCILC).\nIn this paper, we design an exact approach to MCILC based on an exhaustive DPLL\narchitecture. To improve the efficiency, we integrate several effective\nsimplification techniques from mixed integer programming into the architecture.\nWe compare our approach to state-of-the-art MCILC counters and propositional\nmodel counters on 2840 random and 4131 application benchmarks. Experimental\nresults show that our approach significantly outperforms all exact methods in\nrandom benchmarks solving 1718 instances while the state-of-the-art approach\nonly computes 1470 instances. In addition, our approach is the only approach to\nsolve all 4131 application instances.",
      "link": "http://arxiv.org/abs/2509.13880v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Improving cosmological reach of a gravitational wave observatory using\n  Deep Loop Shaping",
      "authors": "Jonas Buchli, Brendan Tracey, Tomislav Andric, Christopher Wipf, Yu Him Justin Chiu, Matthias Lochbrunner, Craig Donner, Rana X. Adhikari, Jan Harms, Iain Barr, Roland Hafner, Andrea Huber, Abbas Abdolmaleki, Charlie Beattie, Joseph Betzwieser, Serkan Cabi, Jonas Degrave, Yuzhu Dong, Leslie Fritz, Anchal Gupta, Oliver Groth, Sandy Huang, Tamara Norman, Hannah Openshaw, Jameson Rollins, Greg Thornton, George Van Den Driessche, Markus Wulfmeier, Pushmeet Kohli, Martin Riedmiller, LIGO Instrument Team",
      "journal": "arXiv",
      "snippet": "Improved low-frequency sensitivity of gravitational wave observatories would\nunlock study of intermediate-mass black hole mergers, binary black hole\neccentricity, and provide early warnings for multi-messenger observations of\nbinary neutron star mergers. Today's mirror stabilization control injects\nharmful noise, constituting a major obstacle to sensitivity improvements. We\neliminated this noise through Deep Loop Shaping, a reinforcement learning\nmethod using frequency domain rewards. We proved our methodology on the LIGO\nLivingston Observatory (LLO). Our controller reduced control noise in the\n10--30Hz band by over 30x, and up to 100x in sub-bands surpassing the design\ngoal motivated by the quantum limit. These results highlight the potential of\nDeep Loop Shaping to improve current and future GW observatories, and more\nbroadly instrumentation and control systems.",
      "link": "http://arxiv.org/abs/2509.14016v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Deep Learning-Driven Peptide Classification in Biological Nanopores",
      "authors": "Samuel Tovey, Julian Hoßbach, Sandro Kuppel, Tobias Ensslen, Jan C. Behrends, Christian Holm",
      "journal": "arXiv",
      "snippet": "A device capable of performing real time classification of proteins in a\nclinical setting would allow for inexpensive and rapid disease diagnosis. One\nsuch candidate for this technology are nanopore devices. These devices work by\nmeasuring a current signal that arises when a protein or peptide enters a\nnanometer-length-scale pore. Should this current be uniquely related to the\nstructure of the peptide and its interactions with the pore, the signals can be\nused to perform identification. While such a method would allow for real time\nidentification of peptides and proteins in a clinical setting, to date, the\ncomplexities of these signals limit their accuracy. In this work, we tackle the\nissue of classification by converting the current signals into scaleogram\nimages via wavelet transforms, capturing amplitude, frequency, and time\ninformation in a modality well-suited to machine learning algorithms. When\ntested on 42 peptides, our method achieved a classification accuracy of\n~$81\\,\\%$, setting a new state-of-the-art in the field and taking a step toward\npractical peptide/protein diagnostics at the point of care. In addition, we\ndemonstrate model transfer techniques that will be critical when deploying\nthese models into real hardware, paving the way to a new method for real-time\ndisease diagnosis.",
      "link": "http://arxiv.org/abs/2509.14029v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based\n  Instruction Recommendation",
      "authors": "Zhipeng Bian, Jieming Zhu, Xuyang Xie, Quanyu Dai, Zhou Zhao, Zhenhua Dong",
      "journal": "arXiv",
      "snippet": "The rapid advancement of generative AI technologies is driving the\nintegration of diverse AI-powered services into smartphones, transforming how\nusers interact with their devices. To simplify access to predefined AI\nservices, this paper introduces MIRA, a pioneering framework for task\ninstruction recommendation that enables intuitive one-touch AI tasking on\nsmartphones. With MIRA, users can long-press on images or text objects to\nreceive contextually relevant instruction recommendations for executing AI\ntasks. Our work introduces three key innovations: 1) A multimodal large\nlanguage model (MLLM)-based recommendation pipeline with structured reasoning\nto extract key entities, infer user intent, and generate precise instructions;\n2) A template-augmented reasoning mechanism that integrates high-level\nreasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based\nconstrained decoding strategy that restricts outputs to predefined instruction\ncandidates, ensuring coherent and intent-aligned suggestions. Through\nevaluation using a real-world annotated datasets and a user study, MIRA has\ndemonstrated substantial improvements in the accuracy of instruction\nrecommendation. The encouraging results highlight MIRA's potential to\nrevolutionize the way users engage with AI services on their smartphones,\noffering a more seamless and efficient experience.",
      "link": "http://arxiv.org/abs/2509.13773v1",
      "published_date": "September 17, 2025",
      "published_date_sort": "2025-09-17",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Reinforcement Learning for Machine Learning Engineering Agents",
      "authors": "S Yang, J He-Yueya, P Liang - arXiv preprint arXiv:2509.01684, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Existing agents for solving tasks such as ML engineering rely on prompting powerful language models. As a result, these agents do not improve with more experience. In this …",
      "link": "https://arxiv.org/abs/2509.01684",
      "published_date": "September 2025",
      "published_date_sort": "2025-09-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "An AI system to help scientists write expert-level empirical software",
      "authors": "E Aygün, A Belyaeva, G Comanici, M Coram… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The cycle of scientific discovery is frequently bottlenecked by the slow, manual creation of software to support computational experiments. To address this, we present an AI system …",
      "link": "https://arxiv.org/abs/2509.06503",
      "published_date": "September 2025",
      "published_date_sort": "2025-09-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement",
      "authors": "J Wang, Y Chen, M Pan, CCM Yeh, M Das - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Coding agents powered by large language models (LLMs) have gained traction for automating code generation through iterative problem-solving with minimal human …",
      "link": "https://arxiv.org/abs/2508.12555",
      "published_date": "August 2025",
      "published_date_sort": "2025-08-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors",
      "authors": "Y Chen, P Piȩkos, M Ostaszewski, F Laakom… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior …",
      "link": "https://arxiv.org/abs/2507.15550",
      "published_date": "July 2025",
      "published_date_sort": "2025-07-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "How Far Are AI Scientists from Changing the World?",
      "authors": "Q Xie, Y Weng, M Zhu, F Shen, S Huang, Z Lin… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of large language models (LLMs) is propelling automated scientific discovery to the next level, with LLM-based Artificial Intelligence (AI) Scientist systems now …",
      "link": "https://arxiv.org/abs/2507.23276",
      "published_date": "July 2025",
      "published_date_sort": "2025-07-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
      "authors": "B Zhao, D Magka, M Jiang, X Li, R Raileanu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce …",
      "link": "https://arxiv.org/abs/2506.22419",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "authors": "Y Ou, Y Luo, J Zheng, L Wei, S Qiao, J Zhang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire …",
      "link": "https://arxiv.org/abs/2506.10974",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement",
      "authors": "J Nam, J Yoon, J Chen, J Shin, SÖ Arık… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Agents based on large language models (LLMs) for machine learning engineering (MLE) can automatically implement ML models via code generation. However, existing approaches …",
      "link": "https://arxiv.org/abs/2506.15692",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research",
      "authors": "S Yan, R Li, Z Luo, Z Wang, D Li, L Jing, K He… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of …",
      "link": "https://arxiv.org/abs/2506.17335",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Towards Community-Driven Agents for Machine Learning Engineering",
      "authors": "S Li, W Sun, S Li, A Talwalkar, Y Yang - arXiv preprint arXiv:2506.20640, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given …",
      "link": "https://arxiv.org/abs/2506.20640",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AI Scientists Fail Without Strong Implementation Capability",
      "authors": "M Zhu, Q Xie, Y Weng, J Wu, Z Lin, L Yang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the …",
      "link": "https://arxiv.org/abs/2506.01372",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 5,
      "institutions": "Unknown"
    },
    {
      "title": "A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications",
      "authors": "R Xu, J Peng - arXiv preprint arXiv:2506.12594, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "This survey examines the rapidly evolving field of Deep Research systems--AI-powered applications that automate complex research workflows through the integration of large …",
      "link": "https://arxiv.org/abs/2506.12594",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 9,
      "institutions": "Unknown"
    },
    {
      "title": "Mle-dojo: Interactive environments for empowering llm agents in machine learning engineering",
      "authors": "R Qiang, Y Zhuang, Y Li, R Zhang, C Li… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative …",
      "link": "https://arxiv.org/abs/2505.07782",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 2,
      "institutions": "Unknown"
    },
    {
      "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving",
      "authors": "H Wang, Z Ni, S Zhang, S Lu, S Hu, Z He, C Hu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world …",
      "link": "https://arxiv.org/abs/2505.21577",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "From automation to autonomy: A survey on large language models in scientific discovery",
      "authors": "T Zheng, Z Deng, HT Tsang, W Wang, J Bai… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and …",
      "link": "https://arxiv.org/abs/2505.13259",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 9,
      "institutions": "Unknown"
    },
    {
      "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
      "authors": "S Li, T Marwah, J Shen, W Sun, A Risteski… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert …",
      "link": "https://arxiv.org/abs/2505.08783",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 3,
      "institutions": "Unknown"
    },
    {
      "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis",
      "authors": "H Li, H Liu, T Zhu, T Guo, Z Zheng, X Deng… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with …",
      "link": "https://arxiv.org/abs/2505.18223",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Mlzero: A multi-agent system for end-to-end machine learning automation",
      "authors": "H Fang, B Han, N Erickson, X Zhang, S Zhou… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly …",
      "link": "https://arxiv.org/abs/2505.13941",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Co-bench: Benchmarking language model agents in algorithm search for combinatorial optimization",
      "authors": "W Sun, S Feng, S Li, Y Yang - arXiv preprint arXiv:2504.04310, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial …",
      "link": "https://arxiv.org/abs/2504.04310",
      "published_date": "April 2025",
      "published_date_sort": "2025-04-01",
      "citations": 9,
      "institutions": "Unknown"
    },
    {
      "title": "Measuring ai ability to complete long tasks",
      "authors": "T Kwa, B West, J Becker, A Deng, K Garcia… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human …",
      "link": "https://arxiv.org/abs/2503.14499",
      "published_date": "March 2025",
      "published_date_sort": "2025-03-01",
      "citations": 32,
      "institutions": "Unknown"
    }
  ]
}