{
  "last_updated": "2025-09-11 00:10:37 UTC",
  "target_paper": "https://www.arxiv.org/abs/2502.13138",
  "arxiv_id": "2502.13138",
  "total_citations": 28,
  "h_index": 4,
  "recent_citations": 9,
  "avg_citations_per_month": "4.00",
  "timeline": [
    {
      "date": "2025-03-01",
      "citations": 1
    },
    {
      "date": "2025-04-01",
      "citations": 2
    },
    {
      "date": "2025-05-01",
      "citations": 8
    },
    {
      "date": "2025-06-01",
      "citations": 15
    },
    {
      "date": "2025-07-01",
      "citations": 17
    },
    {
      "date": "2025-08-01",
      "citations": 19
    },
    {
      "date": "2025-09-01",
      "citations": 28
    }
  ],
  "papers": [
    {
      "title": "$ΔL$ Normalization: Rethink Loss Aggregation in RLVR",
      "authors": "Zhiyuan He, Xufang Luo, Yike Zhang, Yuqing Yang, Lili Qiu",
      "journal": "arXiv",
      "snippet": "We propose $\\Delta L$ Normalization, a simple yet effective loss aggregation\nmethod tailored to the characteristic of dynamic generation lengths in\nReinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has\ndemonstrated strong potential in improving the reasoning capabilities of large\nlanguage models (LLMs), but a major challenge lies in the large variability of\nresponse lengths during training, which leads to high gradient variance and\nunstable optimization. Although previous methods such as GRPO, DAPO, and Dr.\nGRPO introduce different loss normalization terms to address this issue, they\neither produce biased estimates or still suffer from high gradient variance. By\nanalyzing the effect of varying lengths on policy loss both theoretically and\nempirically, we reformulate the problem as finding a minimum-variance unbiased\nestimator. Our proposed $\\Delta L$ Normalization not only provides an unbiased\nestimate of the true policy loss but also minimizes gradient variance in\ntheory. Extensive experiments show that it consistently achieves superior\nresults across different model sizes, maximum lengths, and tasks. Our code will\nbe made public at https://github.com/zerolllin/Delta-L-Normalization.",
      "link": "http://arxiv.org/abs/2509.07558v1",
      "published_date": "September 09, 2025",
      "published_date_sort": "2025-09-09",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition\n  using Demonstration Retrieval",
      "authors": "Zihan Chen, Lei Shi, Weize Wu, Qiji Zhou, Yue Zhang",
      "journal": "arXiv",
      "snippet": "Many contemporary data-driven research efforts in the natural sciences, such\nas chemistry and materials science, require large-scale, high-performance\nentity recognition from scientific datasets. Large language models (LLMs) have\nincreasingly been adopted to solve the entity recognition task, with the same\ntrend being observed on all-spectrum NLP tasks. The prevailing entity\nrecognition LLMs rely on fine-tuned technology, yet the fine-tuning process\noften incurs significant cost. To achieve a best performance-cost trade-off, we\npropose ALLabel, a three-stage framework designed to select the most\ninformative and representative samples in preparing the demonstrations for LLM\nmodeling. The annotated examples are used to construct a ground-truth retrieval\ncorpus for LLM in-context learning. By sequentially employing three distinct\nactive learning strategies, ALLabel consistently outperforms all baselines\nunder the same annotation budget across three specialized domain datasets.\nExperimental results also demonstrate that selectively annotating only 5\\%-10\\%\nof the dataset with ALLabel can achieve performance comparable to the method\nannotating the entire dataset. Further analyses and ablation studies verify the\neffectiveness and generalizability of our proposal.",
      "link": "http://arxiv.org/abs/2509.07512v1",
      "published_date": "September 09, 2025",
      "published_date_sort": "2025-09-09",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "MoE-Compression: How the Compression Error of Experts Affects the\n  Inference Accuracy of MoE Model?",
      "authors": "Songkai Ma, Zhaorui Zhang, Sheng Di, Benben Liu, Xiaodong Yu, Xiaoyi Lu, Dan Wang",
      "journal": "arXiv",
      "snippet": "With the widespread application of Mixture of Experts (MoE) reasoning models\nin the field of LLM learning, efficiently serving MoE models under limited GPU\nmemory constraints has emerged as a significant challenge. Offloading the\nnon-activated experts to main memory has been identified as an efficient\napproach to address such a problem, while it brings the challenges of\ntransferring the expert between the GPU memory and main memory. We need to\nexplore an efficient approach to compress the expert and analyze how the\ncompression error affects the inference performance.\n  To bridge this gap, we propose employing error-bounded lossy compression\nalgorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby\nreducing data transfer overhead during MoE inference. We conduct extensive\nexperiments across various benchmarks and present a comprehensive analysis of\nhow compression-induced errors in different experts affect overall inference\naccuracy. The results indicate that experts in the shallow layers, which are\nprimarily responsible for the attention mechanism and the transformation of\ninput tokens into vector representations, exhibit minimal degradation in\ninference accuracy when subjected to bounded errors. In contrast, errors in the\nmiddle-layer experts, which are central to model reasoning, significantly\nimpair inference accuracy. Interestingly, introducing bounded errors in the\ndeep-layer experts, which are mainly responsible for instruction following and\noutput integration, can sometimes lead to improvements in inference accuracy.",
      "link": "http://arxiv.org/abs/2509.07727v1",
      "published_date": "September 09, 2025",
      "published_date_sort": "2025-09-09",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Graph-based Integrated Gradients for Explaining Graph Neural Networks",
      "authors": "Lachlan Simpson, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, Hong Gunn Chew",
      "journal": "arXiv",
      "snippet": "Integrated Gradients (IG) is a common explainability technique to address the\nblack-box problem of neural networks. Integrated gradients assumes continuous\ndata. Graphs are discrete structures making IG ill-suited to graphs. In this\nwork, we introduce graph-based integrated gradients (GB-IG); an extension of IG\nto graphs. We demonstrate on four synthetic datasets that GB-IG accurately\nidentifies crucial structural components of the graph used in classification\ntasks. We further demonstrate on three prevalent real-world graph datasets that\nGB-IG outperforms IG in highlighting important features for node classification\ntasks.",
      "link": "http://arxiv.org/abs/2509.07648v1",
      "published_date": "September 09, 2025",
      "published_date_sort": "2025-09-09",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Transformer-Based Approach to Optimal Sensor Placement for Structural\n  Health Monitoring of Probe Cards",
      "authors": "Mehdi Bejani, Marco Mauri, Daniele Acconcia, Simone Todaro, Stefano Mariani",
      "journal": "arXiv",
      "snippet": "This paper presents an innovative Transformer-based deep learning strategy\nfor optimizing the placement of sensors aiming at structural health monitoring\nof semiconductor probe cards. Failures in probe cards, including substrate\ncracks and loosened screws, would critically affect semiconductor manufacturing\nyield and reliability. Some failure modes could be detected by equipping a\nprobe card with adequate sensors. Frequency response functions from simulated\nfailure scenarios are adopted within a finite element model of a probe card. A\ncomprehensive dataset, enriched by physics-informed scenario expansion and\nphysics-aware statistical data augmentation, is exploited to train a hybrid\nConvolutional Neural Network and Transformer model. The model achieves high\naccuracy (99.83%) in classifying the probe card health states (baseline, loose\nscrew, crack) and an excellent crack detection recall (99.73%). Model\nrobustness is confirmed through a rigorous framework of 3 repetitions of\n10-fold stratified cross-validation. The attention mechanism also pinpoints\ncritical sensor locations: an analysis of the attention weights offers\nactionable insights for designing efficient, cost-effective monitoring systems\nby optimizing sensor configurations. This research highlights the capability of\nattention-based deep learning to advance proactive maintenance, enhancing\noperational reliability and yield in semiconductor manufacturing.",
      "link": "http://arxiv.org/abs/2509.07603v1",
      "published_date": "September 09, 2025",
      "published_date_sort": "2025-09-09",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for\n  Streamlining Food Carbon Footprint Analysis",
      "authors": "Mustafa Kaan Aslan, Reinout Heijungs, Filip Ilievski",
      "journal": "arXiv",
      "snippet": "Environmental sustainability, particularly in relation to climate change, is\na key concern for consumers, producers, and policymakers. The carbon footprint,\nbased on greenhouse gas emissions, is a standard metric for quantifying the\ncontribution to climate change of activities and is often assessed using life\ncycle assessment (LCA). However, conducting LCA is complex due to opaque and\nglobal supply chains, as well as fragmented data. This paper presents a\nmethodology that combines advances in LCA and publicly available databases with\nknowledge-augmented AI techniques, including retrieval-augmented generation, to\nestimate cradle-to-gate carbon footprints of food products. We introduce a\nchatbot interface that allows users to interactively explore the carbon impact\nof composite meals and relate the results to familiar activities. A live web\ndemonstration showcases our proof-of-concept system with arbitrary food items\nand follow-up questions, highlighting both the potential and limitations - such\nas database uncertainties and AI misinterpretations - of delivering LCA\ninsights in an accessible format.",
      "link": "http://arxiv.org/abs/2509.07733v1",
      "published_date": "September 09, 2025",
      "published_date_sort": "2025-09-09",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Building causation links in stochastic nonlinear systems from data",
      "authors": "Sergio Chibbaro, Cyril Furtlehner, Théo Marchetta, Andrei-Tiberiu Pantea, Davide Rossetti",
      "journal": "arXiv",
      "snippet": "Causal relationships play a fundamental role in understanding the world\naround us. The ability to identify and understand cause-effect relationships is\ncritical to making informed decisions, predicting outcomes, and developing\neffective strategies. However, deciphering causal relationships from\nobservational data is a difficult task, as correlations alone may not provide\ndefinitive evidence of causality. In recent years, the field of machine\nlearning (ML) has emerged as a powerful tool, offering new opportunities for\nuncovering hidden causal mechanisms and better understanding complex systems.\nIn this work, we address the issue of detecting the intrinsic causal links of a\nlarge class of complex systems in the framework of the response theory in\nphysics. We develop some theoretical ideas put forward by [1], and technically\nwe use state-of-the-art ML techniques to build up models from data. We consider\nboth linear stochastic and non-linear systems. Finally, we compute the\nasymptotic efficiency of the linear response based causal predictor in a case\nof large scale Markov process network of linear interactions.",
      "link": "http://arxiv.org/abs/2509.07701v1",
      "published_date": "September 09, 2025",
      "published_date_sort": "2025-09-09",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth\n  Segmentation",
      "authors": "Moo Hyun Son, Juyoung Bae, Zelin Qiu, Jiale Peng, Kai Xin Li, Yifan Lin, Hao Chen",
      "journal": "arXiv",
      "snippet": "Digital dentistry represents a transformative shift in modern dental\npractice. The foundational step in this transformation is the accurate digital\nrepresentation of the patient's dentition, which is obtained from segmented\nCone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the\ngrowing interest in digital dental technologies, existing segmentation\nmethodologies frequently lack rigorous validation and demonstrate limited\nperformance and clinical applicability. To the best of our knowledge, this is\nthe first work to introduce a multimodal pretraining framework for tooth\nsegmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for\npretraining that integrates volumetric (CBCT) and surface-based (IOS)\nmodalities. By capturing modality-invariant representations through multimodal\ncontrastive learning, our approach effectively models fine-grained anatomical\nfeatures, enabling precise multi-class segmentation and accurate identification\nof F\\'ed\\'eration Dentaire Internationale (FDI) tooth numbering. Along with the\nframework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to\ndate, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive\ncollection of independent datasets, representing the largest and most diverse\nevaluation to date. Our method achieves state-of-the-art performance in both\ninternal and external testing, with an increase of 12\\% for CBCT segmentation\nand 8\\% for IOS segmentation in the Dice Similarity Coefficient (DSC).\nFurthermore, ToothMCL consistently surpasses existing approaches in tooth\ngroups and demonstrates robust generalizability across varying imaging\nconditions and clinical scenarios.",
      "link": "http://arxiv.org/abs/2509.07923v1",
      "published_date": "September 09, 2025",
      "published_date_sort": "2025-09-09",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Reinforcement Learning for Machine Learning Engineering Agents",
      "authors": "S Yang, J He-Yueya, P Liang - arXiv preprint arXiv:2509.01684, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Existing agents for solving tasks such as ML engineering rely on prompting powerful language models. As a result, these agents do not improve with more experience. In this …",
      "link": "https://arxiv.org/abs/2509.01684",
      "published_date": "September 2025",
      "published_date_sort": "2025-09-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective",
      "authors": "J Gong, R Giavrimis, P Brookes, V Voskanyan… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "There is a growing interest in leveraging large language models (LLMs) for automated code optimization. However, industrial platforms deploying multiple LLMs face a critical challenge …",
      "link": "https://arxiv.org/abs/2508.01443",
      "published_date": "August 2025",
      "published_date_sort": "2025-08-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement",
      "authors": "J Wang, Y Chen, M Pan, CCM Yeh, M Das - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Coding agents powered by large language models (LLMs) have gained traction for automating code generation through iterative problem-solving with minimal human …",
      "link": "https://arxiv.org/abs/2508.12555",
      "published_date": "August 2025",
      "published_date_sort": "2025-08-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors",
      "authors": "Y Chen, P Piȩkos, M Ostaszewski, F Laakom… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior …",
      "link": "https://arxiv.org/abs/2507.15550",
      "published_date": "July 2025",
      "published_date_sort": "2025-07-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "How Far Are AI Scientists from Changing the World?",
      "authors": "Q Xie, Y Weng, M Zhu, F Shen, S Huang, Z Lin… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of large language models (LLMs) is propelling automated scientific discovery to the next level, with LLM-based Artificial Intelligence (AI) Scientist systems now …",
      "link": "https://arxiv.org/abs/2507.23276",
      "published_date": "July 2025",
      "published_date_sort": "2025-07-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Towards Community-Driven Agents for Machine Learning Engineering",
      "authors": "S Li, W Sun, S Li, A Talwalkar, Y Yang - arXiv preprint arXiv:2506.20640, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given …",
      "link": "https://arxiv.org/abs/2506.20640",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications",
      "authors": "R Xu, J Peng - arXiv preprint arXiv:2506.12594, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "This survey examines the rapidly evolving field of Deep Research systems--AI-powered applications that automate complex research workflows through the integration of large …",
      "link": "https://arxiv.org/abs/2506.12594",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 7,
      "institutions": "Unknown"
    },
    {
      "title": "AI Scientists Fail Without Strong Implementation Capability",
      "authors": "M Zhu, Q Xie, Y Weng, J Wu, Z Lin, L Yang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the …",
      "link": "https://arxiv.org/abs/2506.01372",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 4,
      "institutions": "Unknown"
    },
    {
      "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research",
      "authors": "S Yan, R Li, Z Luo, Z Wang, D Li, L Jing, K He… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of …",
      "link": "https://arxiv.org/abs/2506.17335",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
      "authors": "B Zhao, D Magka, M Jiang, X Li, R Raileanu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce …",
      "link": "https://arxiv.org/abs/2506.22419",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement",
      "authors": "J Nam, J Yoon, J Chen, J Shin, SÖ Arık… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Agents based on large language models (LLMs) for machine learning engineering (MLE) can automatically implement ML models via code generation. However, existing approaches …",
      "link": "https://arxiv.org/abs/2506.15692",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "authors": "Y Ou, Y Luo, J Zheng, L Wei, S Qiao, J Zhang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire …",
      "link": "https://arxiv.org/abs/2506.10974",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Mlzero: A multi-agent system for end-to-end machine learning automation",
      "authors": "H Fang, B Han, N Erickson, X Zhang, S Zhou… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly …",
      "link": "https://arxiv.org/abs/2505.13941",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "From automation to autonomy: A survey on large language models in scientific discovery",
      "authors": "T Zheng, Z Deng, HT Tsang, W Wang, J Bai… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and …",
      "link": "https://arxiv.org/abs/2505.13259",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 9,
      "institutions": "Unknown"
    },
    {
      "title": "Mle-dojo: Interactive environments for empowering llm agents in machine learning engineering",
      "authors": "R Qiang, Y Zhuang, Y Li, R Zhang, C Li… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative …",
      "link": "https://arxiv.org/abs/2505.07782",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis",
      "authors": "H Li, H Liu, T Zhu, T Guo, Z Zheng, X Deng… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with …",
      "link": "https://arxiv.org/abs/2505.18223",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
      "authors": "S Li, T Marwah, J Shen, W Sun, A Risteski… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert …",
      "link": "https://arxiv.org/abs/2505.08783",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving",
      "authors": "H Wang, Z Ni, S Zhang, S Lu, S Hu, Z He, C Hu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world …",
      "link": "https://arxiv.org/abs/2505.21577",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Co-bench: Benchmarking language model agents in algorithm search for combinatorial optimization",
      "authors": "W Sun, S Feng, S Li, Y Yang - arXiv preprint arXiv:2504.04310, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial …",
      "link": "https://arxiv.org/abs/2504.04310",
      "published_date": "April 2025",
      "published_date_sort": "2025-04-01",
      "citations": 8,
      "institutions": "Unknown"
    },
    {
      "title": "Measuring ai ability to complete long tasks",
      "authors": "T Kwa, B West, J Becker, A Deng, K Garcia… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human …",
      "link": "https://arxiv.org/abs/2503.14499",
      "published_date": "March 2025",
      "published_date_sort": "2025-03-01",
      "citations": 30,
      "institutions": "Unknown"
    }
  ]
}