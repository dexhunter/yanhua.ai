{
  "last_updated": "2025-07-24 00:11:54 UTC",
  "target_paper": "https://www.arxiv.org/abs/2502.13138",
  "arxiv_id": "2502.13138",
  "total_citations": 33,
  "h_index": 3,
  "recent_citations": 13,
  "avg_citations_per_month": "4.71",
  "timeline": [
    {
      "date": "2025-01-01",
      "citations": 3
    },
    {
      "date": "2025-03-01",
      "citations": 4
    },
    {
      "date": "2025-04-01",
      "citations": 5
    },
    {
      "date": "2025-05-01",
      "citations": 12
    },
    {
      "date": "2025-06-01",
      "citations": 20
    },
    {
      "date": "2025-07-01",
      "citations": 33
    }
  ],
  "papers": [
    {
      "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science\n  Reasoning",
      "authors": "Run-Ze Fan, Zengzhi Wang, Pengfei Liu",
      "journal": "arXiv",
      "snippet": "Scientific reasoning is critical for developing AI scientists and supporting\nhuman researchers in advancing the frontiers of natural science discovery.\nHowever, the open-source community has primarily focused on mathematics and\ncoding while neglecting the scientific domain, largely due to the absence of\nopen, large-scale, high-quality, verifiable scientific reasoning datasets. To\nbridge this gap, we first present TextbookReasoning, an open dataset featuring\ntruthful reference answers extracted from 12k university-level scientific\ntextbooks, comprising 650k reasoning questions spanning 7 scientific\ndisciplines. We further introduce MegaScience, a large-scale mixture of\nhigh-quality open-source datasets totaling 1.25 million instances, developed\nthrough systematic ablation studies that evaluate various data selection\nmethodologies to identify the optimal subset for each publicly available\nscientific dataset. Meanwhile, we build a comprehensive evaluation system\ncovering diverse subjects and question types across 15 benchmarks,\nincorporating comprehensive answer extraction strategies to ensure accurate\nevaluation metrics. Our experiments demonstrate that our datasets achieve\nsuperior performance and training efficiency with more concise response lengths\ncompared to existing open-source scientific datasets. Furthermore, we train\nLlama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which\nsignificantly outperform the corresponding official instruct models in average\nperformance. In addition, MegaScience exhibits greater effectiveness for larger\nand stronger models, suggesting a scaling benefit for scientific tuning. We\nrelease our data curation pipeline, evaluation system, datasets, and seven\ntrained models to the community to advance scientific reasoning research.",
      "link": "http://arxiv.org/abs/2507.16812v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up\n  with Industry Demands? A Multivocal Literature Review",
      "authors": "Choro Ulan Uulu, Mikhail Kulyabin, Layan Etaiwi, Nuno Miguel Martins Pacheco, Jan Joosten, Kerstin Röse, Filippos Petridis, Jan Bosch, Helena Holmström Olsson",
      "journal": "arXiv",
      "snippet": "Computer-Aided Engineering (CAE) enables simulation experts to optimize\ncomplex models, but faces challenges in user experience (UX) that limit\nefficiency and accessibility. While artificial intelligence (AI) has\ndemonstrated potential to enhance CAE processes, research integrating these\nfields with a focus on UX remains fragmented. This paper presents a multivocal\nliterature review (MLR) examining how AI enhances UX in CAE software across\nboth academic research and industry implementations. Our analysis reveals\nsignificant gaps between academic explorations and industry applications, with\ncompanies actively implementing LLMs, adaptive UIs, and recommender systems\nwhile academic research focuses primarily on technical capabilities without UX\nvalidation. Key findings demonstrate opportunities in AI-powered guidance,\nadaptive interfaces, and workflow automation that remain underexplored in\ncurrent research. By mapping the intersection of these domains, this study\nprovides a foundation for future work to address the identified research gaps\nand advance the integration of AI to improve CAE user experience.",
      "link": "http://arxiv.org/abs/2507.16586v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Pixel-Resolved Long-Context Learning for Turbulence at Exascale:\n  Resolving Small-scale Eddies Toward the Viscous Limit",
      "authors": "Junqi Yin, Mijanur Palash, M. Paul Laiu, Muralikrishnan Gopalakrishnan Meena, John Gounley, Stephen M. de Bruyn Kops, Feiyi Wang, Ramanan Sankaran, Pei Zhang",
      "journal": "arXiv",
      "snippet": "Turbulence plays a crucial role in multiphysics applications, including\naerodynamics, fusion, and combustion. Accurately capturing turbulence's\nmultiscale characteristics is essential for reliable predictions of\nmultiphysics interactions, but remains a grand challenge even for exascale\nsupercomputers and advanced deep learning models. The extreme-resolution data\nrequired to represent turbulence, ranging from billions to trillions of grid\npoints, pose prohibitive computational costs for models based on architectures\nlike vision transformers. To address this challenge, we introduce a multiscale\nhierarchical Turbulence Transformer that reduces sequence length from billions\nto a few millions and a novel RingX sequence parallelism approach that enables\nscalable long-context learning. We perform scaling and science runs on the\nFrontier supercomputer. Our approach demonstrates excellent performance up to\n1.1 EFLOPS on 32,768 AMD GPUs, with a scaling efficiency of 94%. To our\nknowledge, this is the first AI model for turbulence that can capture\nsmall-scale eddies down to the dissipative range.",
      "link": "http://arxiv.org/abs/2507.16697v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Learning Temporal Abstractions via Variational Homomorphisms in\n  Option-Induced Abstract MDPs",
      "authors": "Chang Li, Yaren Zhang, Haoran Lv, Qiong Cao, Chao Xue, Xiaodong He",
      "journal": "arXiv",
      "snippet": "Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol.",
      "link": "http://arxiv.org/abs/2507.16473v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Rethinking LLM-Based RTL Code Optimization Via Timing Logic\n  Metamorphosis",
      "authors": "Zhihao Xu, Bixin Li, Lulu Wang",
      "journal": "arXiv",
      "snippet": "Register Transfer Level(RTL) code optimization is crucial for achieving high\nperformance and low power consumption in digital circuit design. However,\ntraditional optimization methods often rely on manual tuning and heuristics,\nwhich can be time-consuming and error-prone. Recent studies proposed to\nleverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs\ncan generate optimized code snippets based on natural language descriptions,\npotentially speeding up the optimization process. However, existing approaches\nhave not thoroughly evaluated the effectiveness of LLM-Based code optimization\nmethods for RTL code with complex timing logic. To address this gap, we\nconducted a comprehensive empirical investigation to assess the capability of\nLLM-Based RTL code optimization methods in handling RTL code with complex\ntiming logic. In this study, we first propose a new benchmark for RTL\noptimization evaluation. It comprises four subsets, each corresponding to a\nspecific area of RTL code optimization. Then we introduce a method based on\nmetamorphosis to systematically evaluate the effectiveness of LLM-Based RTL\ncode optimization methods.Our key insight is that the optimization\neffectiveness should remain consistent for semantically equivalent but more\ncomplex code. After intensive experiments, we revealed several key findings.\n(1) LLM-Based RTL optimization methods can effectively optimize logic\noperations and outperform existing compiler-based methods. (2) LLM-Based RTL\noptimization methods do not perform better than existing compiler-based methods\non RTL code with complex timing logic, particularly in timing control flow\noptimization and clock domain optimization. This is primarily attributed to the\nchallenges LLMs face in understanding timing logic in RTL code. Based on these\nfindings, we provide insights for further research in leveraging LLMs for RTL\ncode optimization.",
      "link": "http://arxiv.org/abs/2507.16808v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
      "authors": "Tian Dong, Yan Meng, Shaofeng Li, Guoxing Chen, Zhen Liu, Haojin Zhu",
      "journal": "arXiv",
      "snippet": "Large Language Models (LLMs) are increasingly integrated into daily routines,\nyet they raise significant privacy and safety concerns. Recent research\nproposes collaborative inference, which outsources the early-layer inference to\nensure data locality, and introduces model safety auditing based on inner\nneuron patterns. Both techniques expose the LLM's Internal States (ISs), which\nare traditionally considered irreversible to inputs due to optimization\nchallenges and the highly abstract representations in deep layers. In this\nwork, we challenge this assumption by proposing four inversion attacks that\nsignificantly improve the semantic similarity and token matching rate of\ninverted inputs. Specifically, we first develop two white-box\noptimization-based attacks tailored for low-depth and high-depth ISs. These\nattacks avoid local minima convergence, a limitation observed in prior work,\nthrough a two-phase inversion process. Then, we extend our optimization attack\nunder more practical black-box weight access by leveraging the transferability\nbetween the source and the derived LLMs. Additionally, we introduce a\ngeneration-based attack that treats inversion as a translation task, employing\nan inversion model to reconstruct inputs. Extensive evaluation of short and\nlong prompts from medical consulting and coding assistance datasets and 6 LLMs\nvalidates the effectiveness of our inversion attacks. Notably, a 4,112-token\nlong medical consulting prompt can be nearly perfectly inverted with 86.88 F1\ntoken matching from the middle layer of Llama-3 model. Finally, we evaluate\nfour practical defenses that we found cannot perfectly prevent ISs inversion\nand draw conclusions for future mitigation design.",
      "link": "http://arxiv.org/abs/2507.16372v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "RIS-aided Latent Space Alignment for Semantic Channel Equalization",
      "authors": "Tomás Hüttebräucker, Mario Edoardo Pandolfo, Simone Fiorellino, Emilio Calvanese Strinati, Paolo Di Lorenzo",
      "journal": "arXiv",
      "snippet": "Semantic communication systems introduce a new paradigm in wireless\ncommunications, focusing on transmitting the intended meaning rather than\nensuring strict bit-level accuracy. These systems often rely on Deep Neural\nNetworks (DNNs) to learn and encode meaning directly from data, enabling more\nefficient communication. However, in multi-user settings where interacting\nagents are trained independently-without shared context or joint\noptimization-divergent latent representations across AI-native devices can lead\nto semantic mismatches, impeding mutual understanding even in the absence of\ntraditional transmission errors. In this work, we address semantic mismatch in\nMultiple-Input Multiple-Output (MIMO) channels by proposing a joint physical\nand semantic channel equalization framework that leverages the presence of\nReconfigurable Intelligent Surfaces (RIS). The semantic equalization is\nimplemented as a sequence of transformations: (i) a pre-equalization stage at\nthe transmitter; (ii) propagation through the RIS-aided channel; and (iii) a\npost-equalization stage at the receiver. We formulate the problem as a\nconstrained Minimum Mean Squared Error (MMSE) optimization and propose two\nsolutions: (i) a linear semantic equalization chain, and (ii) a non-linear\nDNN-based semantic equalizer. Both methods are designed to operate under\nsemantic compression in the latent space and adhere to transmit power\nconstraints. Through extensive evaluations, we show that the proposed joint\nequalization strategies consistently outperform conventional, disjoint\napproaches to physical and semantic channel equalization across a broad range\nof scenarios and wireless channel conditions.",
      "link": "http://arxiv.org/abs/2507.16450v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Custom Algorithm-based Fault Tolerance for Attention Layers in\n  Transformers",
      "authors": "Vasileios Titopoulos, Kosmas Alexandridis, Giorgos Dimitrakopoulos",
      "journal": "arXiv",
      "snippet": "Transformers and large language models (LLMs), powered by the attention\nmechanism, have transformed numerous AI applications, driving the need for\nspecialized hardware accelerators. A major challenge in these accelerators is\nefficiently detecting errors caused by random hardware faults. Traditional\nalgorithm-based fault tolerance (ABFT) techniques verify individual matrix\nmultiplications but fall short in handling the full attention mechanism,\nparticularly due to intermediate softmax normalization. This work proposes\nFlash-ABFT, a novel method that computes an online checksum across the entire\nthree-matrix product of query, key and value matrices, of an attention layer,\nincluding the softmax operation, with a single check. This approach\nsignificantly reduces overhead by eliminating redundant checks while\nmaintaining high fault-detection accuracy. Experimental results demonstrate\nthat Flash-ABFT incurs only 5.3% hardware area overhead and less than 1.9%\nenergy overhead, making it a cost-effective and robust solution for error\ndetection in attention accelerators.",
      "link": "http://arxiv.org/abs/2507.16676v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "A Comprehensive Data-centric Overview of Federated Graph Learning",
      "authors": "Zhengyu Wu, Xunkai Li, Yinlin Zhu, Zekai Chen, Guochen Yan, Yanyu Yan, Hao Zhang, Yuming Ai, Xinmo Jin, Rong-Hua Li, Guoren Wang",
      "journal": "arXiv",
      "snippet": "In the era of big data applications, Federated Graph Learning (FGL) has\nemerged as a prominent solution that reconcile the tradeoff between optimizing\nthe collective intelligence between decentralized datasets holders and\npreserving sensitive information to maximum. Existing FGL surveys have\ncontributed meaningfully but largely focus on integrating Federated Learning\n(FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that\nemphasis on methodology and simulated scenarios. Notably, a data centric\nperspective, which systematically examines FGL methods through the lens of data\nproperties and usage, remains unadapted to reorganize FGL research, yet it is\ncritical to assess how FGL studies manage to tackle data centric constraints to\nenhance model performances. This survey propose a two-level data centric\ntaxonomy: Data Characteristics, which categorizes studies based on the\nstructural and distributional properties of datasets used in FGL, and Data\nUtilization, which analyzes the training procedures and techniques employed to\novercome key data centric challenges. Each taxonomy level is defined by three\northogonal criteria, each representing a distinct data centric configuration.\nBeyond taxonomy, this survey examines FGL integration with Pretrained Large\nModels, showcases realistic applications, and highlights future direction\naligned with emerging trends in GML.",
      "link": "http://arxiv.org/abs/2507.16541v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading\n  with Multi-Agent Reinforcement Learning",
      "authors": "Mian Ibad Ali Shah, Enda Barrett, Karl Mason",
      "journal": "arXiv",
      "snippet": "This paper presents a novel framework for Peer-to-Peer (P2P) energy trading\nthat integrates uncertainty-aware prediction with multi-agent reinforcement\nlearning (MARL), addressing a critical gap in current literature. In contrast\nto previous works relying on deterministic forecasts, the proposed approach\nemploys a heteroscedastic probabilistic transformer-based prediction model\ncalled Knowledge Transformer with Uncertainty (KTU) to explicitly quantify\nprediction uncertainty, which is essential for robust decision-making in the\nstochastic environment of P2P energy trading. The KTU model leverages\ndomain-specific features and is trained with a custom loss function that\nensures reliable probabilistic forecasts and confidence intervals for each\nprediction. Integrating these uncertainty-aware forecasts into the MARL\nframework enables agents to optimize trading strategies with a clear\nunderstanding of risk and variability. Experimental results show that the\nuncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to\n5.7% without P2P trading and 3.2% with P2P trading, while increasing\nelectricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak\nhour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These\nimprovements are even more pronounced when P2P trading is enabled, highlighting\nthe synergy between advanced forecasting and market mechanisms for resilient,\neconomically efficient energy communities.",
      "link": "http://arxiv.org/abs/2507.16796v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of\n  Precursor Additives and Experimental Design",
      "authors": "Xin-De Wang, Zhi-Rui Chen, Peng-Jie Guo, Ze-Feng Gao, Cheng Mu, Zhong-Yi Lu",
      "journal": "arXiv",
      "snippet": "Perovskite solar cells (PSCs) have rapidly emerged as a leading contender in\nnext-generation photovoltaic technologies, owing to their exceptional power\nconversion efficiencies and advantageous material properties. Despite these\nadvances, challenges such as long-term stability, environmental sustainability,\nand scalable manufacturing continue to hinder their commercialization.\nPrecursor additive engineering has shown promise in addressing these issues by\nenhancing both the performance and durability of PSCs. However, the explosive\ngrowth of scientific literature and the complex interplay of materials,\nprocesses, and device architectures make it increasingly difficult for\nresearchers to efficiently access, organize, and utilize domain knowledge in\nthis rapidly evolving field. To address this gap, we introduce Perovskite-R1, a\nspecialized large language model (LLM) with advanced reasoning capabilities\ntailored for the discovery and design of PSC precursor additives. By\nsystematically mining and curating 1,232 high-quality scientific publications\nand integrating a comprehensive library of 33,269 candidate materials, we\nconstructed a domain-specific instruction-tuning dataset using automated\nquestion-answer generation and chain-of-thought reasoning. Fine-tuning the\nQwQ-32B model on this dataset resulted in Perovskite-R1, which can\nintelligently synthesize literature insights and generate innovative and\npractical solutions for defect passivation and the selection of precursor\nadditives. Experimental validation of several model-proposed strategies\nconfirms their effectiveness in improving material stability and performance.\nOur work demonstrates the potential of domain-adapted LLMs in accelerating\nmaterials discovery and provides a closed-loop framework for intelligent,\ndata-driven advancements in perovskite photovoltaic research.",
      "link": "http://arxiv.org/abs/2507.16307v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Scaling Linear Attention with Sparse State Expansion",
      "authors": "Yuqi Pan, Yongqi An, Zheng Li, Yuhong Chou, Ruijie Zhu, Xiaohui Wang, Mingxuan Wang, Jinqiao Wang, Guoqi Li",
      "journal": "arXiv",
      "snippet": "The Transformer architecture, despite its widespread success, struggles with\nlong-context scenarios due to quadratic computation and linear memory growth.\nWhile various linear attention variants mitigate these efficiency constraints\nby compressing context into fixed-size states, they often degrade performance\nin tasks such as in-context retrieval and reasoning. To address this limitation\nand achieve more effective context compression, we propose two key innovations.\nFirst, we introduce a row-sparse update formulation for linear attention by\nconceptualizing state updating as information classification. This enables\nsparse state updates via softmax-based top-$k$ hard classification, thereby\nextending receptive fields and reducing inter-class interference. Second, we\npresent Sparse State Expansion (SSE) within the sparse framework, which expands\nthe contextual state into multiple partitions, effectively decoupling parameter\nsize from state capacity while maintaining the sparse classification paradigm.\nOur design, supported by efficient parallelized implementations, yields\neffective classification and discriminative state representations. We\nextensively validate SSE in both pure linear and hybrid (SSE-H) architectures\nacross language modeling, in-context retrieval, and mathematical reasoning\nbenchmarks. SSE demonstrates strong retrieval performance and scales favorably\nwith state size. Moreover, after reinforcement learning (RL) training, our 2B\nSSE-H model achieves state-of-the-art mathematical reasoning performance among\nsmall reasoning models, scoring 64.7 on AIME24 and 51.3 on AIME25,\nsignificantly outperforming similarly sized open-source Transformers. These\nresults highlight SSE as a promising and efficient architecture for\nlong-context modeling.",
      "link": "http://arxiv.org/abs/2507.16577v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis\n  Technical Report",
      "authors": "Shanghai AI Lab,  :, Xiaoyang Chen, Yunhao Chen, Zeren Chen, Zhiyun Chen, Hanyun Cui, Yawen Duan, Jiaxuan Guo, Qi Guo, Xuhao Hu, Hong Huang, Lige Huang, Chunxiao Li, Juncheng Li, Qihao Lin, Dongrui Liu, Xinmin Liu, Zicheng Liu, Chaochao Lu, Xiaoya Lu, Jingjing Qu, Qibing Ren, Jing Shao, Jingwei Shi, Jingwei Sun, Peng Wang, Weibing Wang, Jia Xu, Lewen Yan, Xiao Yu, Yi Yu, Boxuan Zhang, Jie Zhang, Weichen Zhang, Zhijie Zheng, Tianyi Zhou, Bowen Zhou",
      "journal": "arXiv",
      "snippet": "To understand and identify the unprecedented risks posed by rapidly advancing\nartificial intelligence (AI) models, this report presents a comprehensive\nassessment of their frontier risks. Drawing on the E-T-C analysis (deployment\nenvironment, threat source, enabling capability) from the Frontier AI Risk\nManagement Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks\nin seven areas: cyber offense, biological and chemical risks, persuasion and\nmanipulation, uncontrolled autonomous AI R\\&D, strategic deception and\nscheming, self-replication, and collusion. Guided by the \"AI-$45^\\circ$ Law,\"\nwe evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow\nlines\" (early warning indicators) to define risk zones: green (manageable risk\nfor routine deployment and continuous monitoring), yellow (requiring\nstrengthened mitigations and controlled deployment), and red (necessitating\nsuspension of development and/or deployment). Experimental results show that\nall recent frontier AI models reside in green and yellow zones, without\ncrossing red lines. Specifically, no evaluated models cross the yellow line for\ncyber offense or uncontrolled AI R\\&D risks. For self-replication, and\nstrategic deception and scheming, most models remain in the green zone, except\nfor certain reasoning models in the yellow zone. In persuasion and\nmanipulation, most models are in the yellow zone due to their effective\ninfluence on humans. For biological and chemical risks, we are unable to rule\nout the possibility of most models residing in the yellow zone, although\ndetailed threat modeling and in-depth assessment are required to make further\nclaims. This work reflects our current understanding of AI frontier risks and\nurges collective action to mitigate these challenges.",
      "link": "http://arxiv.org/abs/2507.16534v1",
      "published_date": "July 22, 2025",
      "published_date_sort": "2025-07-22",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
      "authors": "B Zhao, D Magka, M Jiang, X Li, R Raileanu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce …",
      "link": "https://arxiv.org/abs/2506.22419",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Towards Community-Driven Agents for Machine Learning Engineering",
      "authors": "S Li, W Sun, S Li, A Talwalkar, Y Yang - arXiv preprint arXiv:2506.20640, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given …",
      "link": "https://arxiv.org/abs/2506.20640",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "authors": "Y Ou, Y Luo, J Zheng, L Wei, S Qiao, J Zhang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire …",
      "link": "https://arxiv.org/abs/2506.10974",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science",
      "authors": "A Luo, X Xian, J Du, F Tian, G Wang, M Zhong… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as …",
      "link": "https://arxiv.org/abs/2506.13992",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AI Scientists Fail Without Strong Implementation Capability",
      "authors": "M Zhu, Q Xie, Y Weng, J Wu, Z Lin, L Yang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the …",
      "link": "https://arxiv.org/abs/2506.01372",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement",
      "authors": "J Nam, J Yoon, J Chen, J Shin, SÖ Arık… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Agents based on large language models (LLMs) for machine learning engineering (MLE) can automatically implement ML models via code generation. However, existing approaches …",
      "link": "https://arxiv.org/abs/2506.15692",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications",
      "authors": "R Xu, J Peng - arXiv preprint arXiv:2506.12594, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "This survey examines the rapidly evolving field of Deep Research systems--AI-powered applications that automate complex research workflows through the integration of large …",
      "link": "https://arxiv.org/abs/2506.12594",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research",
      "authors": "S Yan, R Li, Z Luo, Z Wang, D Li, L Jing, K He… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of …",
      "link": "https://arxiv.org/abs/2506.17335",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents",
      "authors": "Y Cai, X Li, M Goswami, M Wiliński, G Welter… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating Artificial Intelligence (AI) agents on time series machine learning engineering challenges. Existing …",
      "link": "https://arxiv.org/abs/2505.13291",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Mle-dojo: Interactive environments for empowering llm agents in machine learning engineering",
      "authors": "R Qiang, Y Zhuang, Y Li, R Zhang, C Li… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative …",
      "link": "https://arxiv.org/abs/2505.07782",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving",
      "authors": "H Wang, Z Ni, S Zhang, S Lu, S Hu, Z He, C Hu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world …",
      "link": "https://arxiv.org/abs/2505.21577",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis",
      "authors": "H Li, H Liu, T Zhu, T Guo, Z Zheng, X Deng… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with …",
      "link": "https://arxiv.org/abs/2505.18223",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Mlzero: A multi-agent system for end-to-end machine learning automation",
      "authors": "H Fang, B Han, N Erickson, X Zhang, S Zhou… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly …",
      "link": "https://arxiv.org/abs/2505.13941",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
      "authors": "S Li, T Marwah, J Shen, W Sun, A Risteski… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert …",
      "link": "https://arxiv.org/abs/2505.08783",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "From automation to autonomy: A survey on large language models in scientific discovery",
      "authors": "T Zheng, Z Deng, HT Tsang, W Wang, J Bai… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and …",
      "link": "https://arxiv.org/abs/2505.13259",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 3,
      "institutions": "Unknown"
    },
    {
      "title": "Co-bench: Benchmarking language model agents in algorithm search for combinatorial optimization",
      "authors": "W Sun, S Feng, S Li, Y Yang - arXiv preprint arXiv:2504.04310, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial …",
      "link": "https://arxiv.org/abs/2504.04310",
      "published_date": "April 2025",
      "published_date_sort": "2025-04-01",
      "citations": 6,
      "institutions": "Unknown"
    },
    {
      "title": "Measuring ai ability to complete long tasks",
      "authors": "T Kwa, B West, J Becker, A Deng, K Garcia… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human …",
      "link": "https://arxiv.org/abs/2503.14499",
      "published_date": "March 2025",
      "published_date_sort": "2025-03-01",
      "citations": 21,
      "institutions": "Unknown"
    },
    {
      "title": "PiML: Automated Machine Learning Workflow Optimization using LLM Agents",
      "authors": "A Chopde, F Pettiwala, S Kirubananth, SK Botla… - AutoML 2025 Methods … - openreview.net",
      "journal": "Google Scholar Result",
      "snippet": "In this paper, we introduce PiML-Persistent Iterative Machine Learning agentic framework, a novel automated pipeline specifically designed for solving real-world machine learning (ML) …",
      "link": "https://openreview.net/forum?id=Nw1qBpsjZz",
      "published_date": "2025",
      "published_date_sort": "2025-01-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Towards Pragmatic Time Series Intelligence",
      "authors": "M Goswami - 2025 - search.proquest.com",
      "journal": "Google Scholar Result",
      "snippet": "This thesis aims to democratize time series intelligence by making advanced modeling capabilities accessible to users without specialized machine learning knowledge. We …",
      "link": "https://search.proquest.com/openview/24ff18df8583af7a835b667b59b216b9/1?pq-origsite=gscholar&cbl=18750&diss=y",
      "published_date": "2025",
      "published_date_sort": "2025-01-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AutoCluster: Un agent pour le clustering basé sur les grands modèles de langue",
      "authors": "E Versmée, Y Remil, M Kaytoue, J Velcin - coria-taln-2025.lis-lab.fr",
      "journal": "Google Scholar Result",
      "snippet": "Cette recherche présente AutoCluster, un agent basé sur les grands modèles de langue pour des tâches de classification non supervisée. Nous concevons trois agents dont deux …",
      "link": "https://coria-taln-2025.lis-lab.fr/wp-content/uploads/2025/06/CORIA-TALN_2025_paper_124.pdf",
      "published_date": "2025",
      "published_date_sort": "2025-01-01",
      "citations": 0,
      "institutions": "Unknown"
    }
  ]
}