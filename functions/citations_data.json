{
  "last_updated": "2025-08-16 00:10:59 UTC",
  "target_paper": "https://www.arxiv.org/abs/2502.13138",
  "arxiv_id": "2502.13138",
  "total_citations": 30,
  "h_index": 4,
  "recent_citations": 11,
  "avg_citations_per_month": "5.00",
  "timeline": [
    {
      "date": "2025-03-01",
      "citations": 1
    },
    {
      "date": "2025-04-01",
      "citations": 2
    },
    {
      "date": "2025-05-01",
      "citations": 8
    },
    {
      "date": "2025-06-01",
      "citations": 16
    },
    {
      "date": "2025-07-01",
      "citations": 19
    },
    {
      "date": "2025-08-01",
      "citations": 30
    }
  ],
  "papers": [
    {
      "title": "FROGENT: An End-to-End Full-process Drug Design Agent",
      "authors": "Qihua Pan, Dong Xu, Jenna Xinyi Yao, Lijia Ma, Zexuan Zhu, Junkai Ji",
      "journal": "arXiv",
      "snippet": "Powerful AI tools for drug discovery reside in isolated web apps, desktop\nprograms, and code libraries. Such fragmentation forces scientists to manage\nincompatible interfaces and specialized scripts, which can be a cumbersome and\nrepetitive process. To address this issue, a Full-pROcess druG dEsign ageNT,\nnamed FROGENT, has been proposed. Specifically, FROGENT utilizes a Large\nLanguage Model and the Model Context Protocol to integrate multiple dynamic\nbiochemical databases, extensible tool libraries, and task-specific AI models.\nThis agentic framework allows FROGENT to execute complicated drug discovery\nworkflows dynamically, including component tasks such as target identification,\nmolecule generation and retrosynthetic planning. FROGENT has been evaluated on\neight benchmarks that cover various aspects of drug discovery, such as\nknowledge retrieval, property prediction, virtual screening, mechanistic\nanalysis, molecular design, and synthesis. It was compared against six\nincreasingly advanced ReAct-style agents that support code execution and\nliterature searches. Empirical results demonstrated that FROGENT triples the\nbest baseline performance in hit-finding and doubles it in interaction\nprofiling, significantly outperforming both the open-source model Qwen3-32B and\nthe commercial model GPT-4o. In addition, real-world cases have been utilized\nto validate the practicability and generalization of FROGENT. This development\nsuggests that streamlining the agentic drug discovery pipeline can\nsignificantly enhance researcher productivity.",
      "link": "http://arxiv.org/abs/2508.10760v1",
      "published_date": "August 14, 2025",
      "published_date_sort": "2025-08-14",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis\n  Driven by a Large Language Model",
      "authors": "Shicheng Xu, Xin Huang, Zihao Wei, Liang Pang, Huawei Shen, Xueqi Cheng",
      "journal": "arXiv",
      "snippet": "Full-process clinical diagnosis in the real world encompasses the entire\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\nartificial intelligence (AI), particularly large language models (LLMs), is\ntransforming clinical diagnosis, its role remains largely as an assistant to\nphysicians. This AI-assisted working pattern makes AI can only answer specific\nmedical questions at certain parts within the diagnostic process, but lack the\nability to drive the entire diagnostic process starting from an ambiguous\ncomplaint, which still relies heavily on human physicians. This gap limits AI's\nability to fully reduce physicians' workload and enhance diagnostic efficiency.\nTo address this, we propose a paradigm shift that reverses the relationship\nbetween physicians and AI: repositioning AI as the primary director, with\nphysicians serving as its assistants. So we present DxDirector-7B, an LLM\nendowed with advanced deep thinking capabilities, enabling it to drive the\nfull-process diagnosis with minimal physician involvement. Furthermore,\nDxDirector-7B establishes a robust accountability framework for misdiagnoses,\ndelineating responsibility between AI and human physicians. In evaluations\nacross rare, complex, and real-world cases under full-process diagnosis\nsetting, DxDirector-7B not only achieves significant superior diagnostic\naccuracy but also substantially reduces physician workload than\nstate-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained\nanalyses across multiple clinical departments and tasks validate its efficacy,\nwith expert evaluations indicating its potential to serve as a viable\nsubstitute for medical specialists. These findings mark a new era where AI,\ntraditionally a physicians' assistant, now drives the entire diagnostic process\nto drastically reduce physicians' workload, indicating an efficient and\naccurate diagnostic solution.",
      "link": "http://arxiv.org/abs/2508.10492v1",
      "published_date": "August 14, 2025",
      "published_date_sort": "2025-08-14",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation",
      "authors": "Lisa Haxel, Jaivardhan Kapoor, Ulf Ziemann, Jakob H. Macke",
      "journal": "arXiv",
      "snippet": "Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural\nsignals drift over time and vary across users, requiring frequent recalibration\nthat limits practical deployment. We introduce EDAPT, a task- and\nmodel-agnostic framework that eliminates calibration through continual model\nadaptation. EDAPT first trains a baseline decoder using data from multiple\nusers, then continually personalizes this model via supervised finetuning as\nthe neural patterns evolve during use. We tested EDAPT across nine datasets\ncovering three BCI tasks, and found that it consistently improved accuracy over\nconventional, static methods. These improvements primarily stem from combining\npopulation-level pretraining and online continual finetuning, with unsupervised\ndomain adaptation providing further gains on some datasets. EDAPT runs\nefficiently, updating models within 200 milliseconds on consumer-grade\nhardware. Finally, decoding accuracy scales with total data budget rather than\nits allocation between subjects and trials. EDAPT provides a practical pathway\ntoward calibration-free BCIs, reducing a major barrier to BCI deployment.",
      "link": "http://arxiv.org/abs/2508.10474v1",
      "published_date": "August 14, 2025",
      "published_date_sort": "2025-08-14",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "A Dataset for Distilling Knowledge Priors from Literature for\n  Therapeutic Design",
      "authors": "Haydn Thomas Jones, Natalie Maus, Josh Magnus Ludan, Maggie Ziyu Huan, Jiaming Liang, Marcelo Der Torossian Torres, Jiatao Liang, Zachary Ives, Yoseph Barash, Cesar de la Fuente-Nunez, Jacob R. Gardner, Mark Yatskar",
      "journal": "arXiv",
      "snippet": "AI-driven discovery can greatly reduce design time and enhance new\ntherapeutics' effectiveness. Models using simulators explore broad design\nspaces but risk violating implicit constraints due to a lack of experimental\npriors. For example, in a new analysis we performed on a diverse set of models\non the GuacaMol benchmark using supervised classifiers, over 60\\% of molecules\nproposed had high probability of being mutagenic. In this work, we introduce\n\\ourdataset, a dataset of priors for design problems extracted from literature\ndescribing compounds used in lab settings. It is constructed with LLM pipelines\nfor discovering therapeutic entities in relevant paragraphs and summarizing\ninformation in concise fair-use facts. \\ourdataset~ consists of 32.3 million\npairs of natural language facts, and appropriate entity representations (i.e.\nSMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,\nCLIP, and LLava architectures to reason jointly about text and design targets\nand evaluate on tasks from the Therapeutic Data Commons (TDC). \\ourdataset~is\nhighly effective for creating models with strong priors: in supervised\nprediction problems that use our data as pretraining, our best models with 15M\nlearnable parameters outperform larger 2B TxGemma on both regression and\nclassification TDC tasks, and perform comparably to 9B models on average.\nModels built with \\ourdataset~can be used as constraints while optimizing for\nnovel molecules in GuacaMol, resulting in proposals that are safer and nearly\nas effective. We release our dataset at\n\\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},\nand will provide expanded versions as available literature grows.",
      "link": "http://arxiv.org/abs/2508.10899v1",
      "published_date": "August 14, 2025",
      "published_date_sort": "2025-08-14",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and\n  Adaptive Chest X-Ray Reasoning",
      "authors": "Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu",
      "journal": "arXiv",
      "snippet": "Existing tool-augmented agentic systems are limited in the real world by (i)\nblack-box reasoning steps that undermine trust of decision-making and pose\nsafety risks, (ii) poor multimodal integration, which is inherently critical\nfor healthcare tasks, and (iii) rigid and computationally inefficient agentic\npipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the\nfirst multimodal framework to address these challenges in the context of Chest\nX-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a\nmulti-tool graph, yielding decision paths annotated with interpretable\nprobabilities. Given the complex CXR reasoning task with multimodal medical\ndata, PASS leverages its learned task-conditioned distribution over the agentic\nsupernet. Thus, it adaptively selects the most suitable tool at each supernet\nlayer, offering probability-annotated trajectories for post-hoc audits and\ndirectly enhancing medical AI safety. PASS also continuously compresses salient\nfindings into an evolving personalized memory, while dynamically deciding\nwhether to deepen its reasoning path or invoke an early exit for efficiency. To\noptimize a Pareto frontier balancing performance and cost, we design a novel\nthree-stage training procedure, including expert knowledge warm-up, contrastive\npath-ranking, and cost-aware reinforcement learning. To facilitate rigorous\nevaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,\nsafety-critical, free-form CXR reasoning. Experiments across various benchmarks\nvalidate that PASS significantly outperforms strong baselines in multiple\nmetrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,\npushing a new paradigm shift towards interpretable, adaptive, and multimodal\nmedical agentic systems.",
      "link": "http://arxiv.org/abs/2508.10501v1",
      "published_date": "August 14, 2025",
      "published_date_sort": "2025-08-14",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Electromagnetic Simulations of Antennas on GPUs for Machine Learning\n  Applications",
      "authors": "Murat Temiz, Vemund Bakken",
      "journal": "arXiv",
      "snippet": "This study proposes an antenna simulation framework powered by graphics\nprocessing units (GPUs) based on an open-source electromagnetic (EM) simulation\nsoftware (gprMax) for machine learning applications of antenna design and\noptimization. Furthermore, it compares the simulation results with those\nobtained through commercial EM software. The proposed software framework for\nmachine learning and surrogate model applications will produce antenna data\nsets consisting of a large number of antenna simulation results using GPUs.\nAlthough machine learning methods can attain the optimum solutions for many\nproblems, they are known to be data-hungry and require a great deal of samples\nfor the training stage of the algorithms. However, producing a sufficient\nnumber of training samples in EM applications within a limited time is\nchallenging due to the high computational complexity of EM simulations.\nTherefore, GPUs are utilized in this study to simulate a large number of\nantennas with predefined or random antenna shape parameters to produce data\nsets. Moreover, this study also compares various machine learning and deep\nlearning models in terms of antenna parameter estimation performance. This\nstudy demonstrates that an entry-level GPU substantially outperforms a high-end\nCPU in terms of computational performance, while a high-end gaming GPU can\nachieve around 18 times more computational performance compared to a high-end\nCPU. Moreover, it is shown that the open-source EM simulation software can\ndeliver similar results to those obtained via commercial software in the\nsimulation of microstrip antennas when the spatial resolution of the\nsimulations is sufficiently fine.",
      "link": "http://arxiv.org/abs/2508.10713v1",
      "published_date": "August 14, 2025",
      "published_date_sort": "2025-08-14",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Technical Solutions",
      "authors": "Parsa Omidi, Xingshuai Huang, Axel Laborieux, Bahareh Nikpour, Tianyu Shi, Armaghan Eshaghi",
      "journal": "arXiv",
      "snippet": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
      "link": "http://arxiv.org/abs/2508.10824v1",
      "published_date": "August 14, 2025",
      "published_date_sort": "2025-08-14",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models",
      "authors": "Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, Guang Shi",
      "journal": "arXiv",
      "snippet": "Reinforcement learning with verifiable rewards (RLVR), which typically adopts\nPass@1 as the reward, has faced the issues in balancing exploration and\nexploitation, causing policies to prefer conservative actions, converging to a\nlocal optimum. Identifying an appropriate reward metric is therefore crucial.\nRegarding the prior work, although Pass@k has been used in evaluation, its\nconnection to LLM exploration ability in RLVR remains largely overlooked. To\ninvestigate this, we first use Pass@k as the reward to train the policy model\n(i.e., $\\textbf{Pass@k Training}$), and observe the improvement on its\nexploration ability. Next, we derive an analytical solution for the advantage\nof Pass@k Training, leading to an efficient and effective process. Building on\nthis, our analysis reveals that exploration and exploitation are not inherently\nconflicting objectives, while they can mutually enhance each other. Moreover,\nPass@k Training with analytical derivation essentially involves directly\ndesigning the advantage function. Inspired by this, we preliminarily explore\nthe advantage design for RLVR, showing promising results and highlighting a\npotential future direction.",
      "link": "http://arxiv.org/abs/2508.10751v1",
      "published_date": "August 14, 2025",
      "published_date_sort": "2025-08-14",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Conditional Information Bottleneck for Multimodal Fusion: Overcoming\n  Shortcut Learning in Sarcasm Detection",
      "authors": "Yihua Wang, Qi Jia, Cong Xu, Feiyu Chen, Yuhan Liu, Haotian Zhang, Liang Jin, Lu Liu, Zhichun Wang",
      "journal": "arXiv",
      "snippet": "Multimodal sarcasm detection is a complex task that requires distinguishing\nsubtle complementary signals across modalities while filtering out irrelevant\ninformation. Many advanced methods rely on learning shortcuts from datasets\nrather than extracting intended sarcasm-related features. However, our\nexperiments show that shortcut learning impairs the model's generalization in\nreal-world scenarios. Furthermore, we reveal the weaknesses of current modality\nfusion strategies for multimodal sarcasm detection through systematic\nexperiments, highlighting the necessity of focusing on effective modality\nfusion for complex emotion recognition. To address these challenges, we\nconstruct MUStARD++$^{R}$ by removing shortcut signals from MUStARD++. Then, a\nMultimodal Conditional Information Bottleneck (MCIB) model is introduced to\nenable efficient multimodal fusion for sarcasm detection. Experimental results\nshow that the MCIB achieves the best performance without relying on shortcut\nlearning.",
      "link": "http://arxiv.org/abs/2508.10644v1",
      "published_date": "August 14, 2025",
      "published_date_sort": "2025-08-14",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Memorisation and forgetting in a learning Hopfield neural network:\n  bifurcation mechanisms, attractors and basins",
      "authors": "Adam E. Essex, Natalia B. Janson, Rachel A. Norris, Alexander G. Balanov",
      "journal": "arXiv",
      "snippet": "Despite explosive expansion of artificial intelligence based on artificial\nneural networks (ANNs), these are employed as \"black boxes'', as it is unclear\nhow, during learning, they form memories or develop unwanted features,\nincluding spurious memories and catastrophic forgetting. Much research is\navailable on isolated aspects of learning ANNs, but due to their high\ndimensionality and non-linearity, their comprehensive analysis remains a\nchallenge. In ANNs, knowledge is thought to reside in connection weights or in\nattractor basins, but these two paradigms are not linked explicitly. Here we\ncomprehensively analyse mechanisms of memory formation in an 81-neuron Hopfield\nnetwork undergoing Hebbian learning by revealing bifurcations leading to\nformation and destruction of attractors and their basin boundaries. We show\nthat, by affecting evolution of connection weights, the applied stimuli induce\na pitchfork and then a cascade of saddle-node bifurcations creating new\nattractors with their basins that can code true or spurious memories, and an\nabrupt disappearance of old memories (catastrophic forgetting). With successful\nlearning, new categories are represented by the basins of newly born point\nattractors, and their boundaries by the stable manifolds of new saddles. With\nthis, memorisation and forgetting represent two manifestations of the same\nmechanism. Our strategy to analyse high-dimensional learning ANNs is universal\nand applicable to recurrent ANNs of any form. The demonstrated mechanisms of\nmemory formation and of catastrophic forgetting shed light on the operation of\na wider class of recurrent ANNs and could aid the development of approaches to\nmitigate their flaws.",
      "link": "http://arxiv.org/abs/2508.10765v1",
      "published_date": "August 14, 2025",
      "published_date_sort": "2025-08-14",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective",
      "authors": "J Gong, R Giavrimis, P Brookes, V Voskanyan… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "There is a growing interest in leveraging large language models (LLMs) for automated code optimization. However, industrial platforms deploying multiple LLMs face a critical challenge …",
      "link": "https://arxiv.org/abs/2508.01443",
      "published_date": "August 2025",
      "published_date_sort": "2025-08-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors",
      "authors": "Y Chen, P Piȩkos, M Ostaszewski, F Laakom… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior …",
      "link": "https://arxiv.org/abs/2507.15550",
      "published_date": "July 2025",
      "published_date_sort": "2025-07-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data",
      "authors": "A Lapin, I Hromov, S Chumakov, M Mitrovic… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "AutoML has advanced in handling complex tasks using the integration of LLMs, yet its efficiency remains limited by dependence on specific underlying tools. In this paper, we …",
      "link": "https://arxiv.org/abs/2507.13413",
      "published_date": "July 2025",
      "published_date_sort": "2025-07-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "How Far Are AI Scientists from Changing the World?",
      "authors": "Q Xie, Y Weng, M Zhu, F Shen, S Huang, Z Lin… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of large language models (LLMs) is propelling automated scientific discovery to the next level, with LLM-based Artificial Intelligence (AI) Scientist systems now …",
      "link": "https://arxiv.org/abs/2507.23276",
      "published_date": "July 2025",
      "published_date_sort": "2025-07-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research",
      "authors": "S Yan, R Li, Z Luo, Z Wang, D Li, L Jing, K He… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of …",
      "link": "https://arxiv.org/abs/2506.17335",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications",
      "authors": "R Xu, J Peng - arXiv preprint arXiv:2506.12594, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "This survey examines the rapidly evolving field of Deep Research systems--AI-powered applications that automate complex research workflows through the integration of large …",
      "link": "https://arxiv.org/abs/2506.12594",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 4,
      "institutions": "Unknown"
    },
    {
      "title": "Towards Community-Driven Agents for Machine Learning Engineering",
      "authors": "S Li, W Sun, S Li, A Talwalkar, Y Yang - arXiv preprint arXiv:2506.20640, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given …",
      "link": "https://arxiv.org/abs/2506.20640",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AI Scientists Fail Without Strong Implementation Capability",
      "authors": "M Zhu, Q Xie, Y Weng, J Wu, Z Lin, L Yang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the …",
      "link": "https://arxiv.org/abs/2506.01372",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 2,
      "institutions": "Unknown"
    },
    {
      "title": "AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science",
      "authors": "A Luo, X Xian, J Du, F Tian, G Wang, M Zhong… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as …",
      "link": "https://arxiv.org/abs/2506.13992",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement",
      "authors": "J Nam, J Yoon, J Chen, J Shin, SÖ Arık… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Agents based on large language models (LLMs) for machine learning engineering (MLE) can automatically implement ML models via code generation. However, existing approaches …",
      "link": "https://arxiv.org/abs/2506.15692",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "authors": "Y Ou, Y Luo, J Zheng, L Wei, S Qiao, J Zhang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire …",
      "link": "https://arxiv.org/abs/2506.10974",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
      "authors": "B Zhao, D Magka, M Jiang, X Li, R Raileanu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce …",
      "link": "https://arxiv.org/abs/2506.22419",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis",
      "authors": "H Li, H Liu, T Zhu, T Guo, Z Zheng, X Deng… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with …",
      "link": "https://arxiv.org/abs/2505.18223",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
      "authors": "S Li, T Marwah, J Shen, W Sun, A Risteski… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert …",
      "link": "https://arxiv.org/abs/2505.08783",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents",
      "authors": "Y Cai, X Li, M Goswami, M Wiliński, G Welter… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating Artificial Intelligence (AI) agents on time series machine learning engineering challenges. Existing …",
      "link": "https://arxiv.org/abs/2505.13291",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Mlzero: A multi-agent system for end-to-end machine learning automation",
      "authors": "H Fang, B Han, N Erickson, X Zhang, S Zhou… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly …",
      "link": "https://arxiv.org/abs/2505.13941",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "From automation to autonomy: A survey on large language models in scientific discovery",
      "authors": "T Zheng, Z Deng, HT Tsang, W Wang, J Bai… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and …",
      "link": "https://arxiv.org/abs/2505.13259",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 7,
      "institutions": "Unknown"
    },
    {
      "title": "Mle-dojo: Interactive environments for empowering llm agents in machine learning engineering",
      "authors": "R Qiang, Y Zhuang, Y Li, R Zhang, C Li… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative …",
      "link": "https://arxiv.org/abs/2505.07782",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Co-bench: Benchmarking language model agents in algorithm search for combinatorial optimization",
      "authors": "W Sun, S Feng, S Li, Y Yang - arXiv preprint arXiv:2504.04310, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial …",
      "link": "https://arxiv.org/abs/2504.04310",
      "published_date": "April 2025",
      "published_date_sort": "2025-04-01",
      "citations": 8,
      "institutions": "Unknown"
    },
    {
      "title": "Measuring ai ability to complete long tasks",
      "authors": "T Kwa, B West, J Becker, A Deng, K Garcia… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human …",
      "link": "https://arxiv.org/abs/2503.14499",
      "published_date": "March 2025",
      "published_date_sort": "2025-03-01",
      "citations": 23,
      "institutions": "Unknown"
    }
  ]
}