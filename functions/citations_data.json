{
  "last_updated": "2025-07-25 00:11:38 UTC",
  "target_paper": "https://www.arxiv.org/abs/2502.13138",
  "arxiv_id": "2502.13138",
  "total_citations": 34,
  "h_index": 3,
  "recent_citations": 16,
  "avg_citations_per_month": "4.86",
  "timeline": [
    {
      "date": "2025-01-01",
      "citations": 3
    },
    {
      "date": "2025-03-01",
      "citations": 4
    },
    {
      "date": "2025-04-01",
      "citations": 5
    },
    {
      "date": "2025-05-01",
      "citations": 12
    },
    {
      "date": "2025-06-01",
      "citations": 18
    },
    {
      "date": "2025-07-01",
      "citations": 34
    }
  ],
  "papers": [
    {
      "title": "Flow Matching Meets Biology and Life Science: A Survey",
      "authors": "Zihao Li, Zhichen Zeng, Xiao Lin, Feihao Fang, Yanru Qu, Zhe Xu, Zhining Liu, Xuying Ning, Tianxin Wei, Ge Liu, Hanghang Tong, Jingrui He",
      "journal": "arXiv",
      "snippet": "Over the past decade, advances in generative modeling, such as generative\nadversarial networks, masked autoencoders, and diffusion models, have\nsignificantly transformed biological research and discovery, enabling\nbreakthroughs in molecule design, protein generation, drug discovery, and\nbeyond. At the same time, biological applications have served as valuable\ntestbeds for evaluating the capabilities of generative models. Recently, flow\nmatching has emerged as a powerful and efficient alternative to diffusion-based\ngenerative modeling, with growing interest in its application to problems in\nbiology and life sciences. This paper presents the first comprehensive survey\nof recent developments in flow matching and its applications in biological\ndomains. We begin by systematically reviewing the foundations and variants of\nflow matching, and then categorize its applications into three major areas:\nbiological sequence modeling, molecule generation and design, and peptide and\nprotein generation. For each, we provide an in-depth review of recent progress.\nWe also summarize commonly used datasets and software tools, and conclude with\na discussion of potential future directions. The corresponding curated\nresources are available at\nhttps://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.",
      "link": "http://arxiv.org/abs/2507.17731v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Vision Transformer attention alignment with human visual perception in\n  aesthetic object evaluation",
      "authors": "Miguel Carrasco, César González-Martín, José Aranda, Luis Oliveros",
      "journal": "arXiv",
      "snippet": "Visual attention mechanisms play a crucial role in human perception and\naesthetic evaluation. Recent advances in Vision Transformers (ViTs) have\ndemonstrated remarkable capabilities in computer vision tasks, yet their\nalignment with human visual attention patterns remains underexplored,\nparticularly in aesthetic contexts. This study investigates the correlation\nbetween human visual attention and ViT attention mechanisms when evaluating\nhandcrafted objects. We conducted an eye-tracking experiment with 30\nparticipants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal\nobjects comprising basketry bags and ginger jars. Using a Pupil Labs\neye-tracker, we recorded gaze patterns and generated heat maps representing\nhuman visual attention. Simultaneously, we analyzed the same objects using a\npre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting\nattention maps from each of the 12 attention heads. We compared human and ViT\nattention distributions using Kullback-Leibler divergence across varying\nGaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal\ncorrelation at sigma=2.4 +-0.03, with attention head #12 showing the strongest\nalignment with human visual patterns. Significant differences were found\nbetween attention heads, with heads #7 and #9 demonstrating the greatest\ndivergence from human attention (p< 0.05, Tukey HSD test). Results indicate\nthat while ViTs exhibit more global attention patterns compared to human focal\nattention, certain attention heads can approximate human visual behavior,\nparticularly for specific object features like buckles in basketry items. These\nfindings suggest potential applications of ViT attention mechanisms in product\ndesign and aesthetic evaluation, while highlighting fundamental differences in\nattention strategies between human perception and current AI models.",
      "link": "http://arxiv.org/abs/2507.17616v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Reasoning-Driven Retrosynthesis Prediction with Large Language Models\n  via Reinforcement Learning",
      "authors": "Situo Zhang, Hanqi Li, Lu Chen, Zihan Zhao, Xuanze Lin, Zichen Zhu, Bo Chen, Xin Chen, Kai Yu",
      "journal": "arXiv",
      "snippet": "Retrosynthesis planning, essential in organic synthesis and drug discovery,\nhas greatly benefited from recent AI-driven advancements. Nevertheless,\nexisting methods frequently face limitations in both applicability and\nexplainability. Traditional graph-based and sequence-to-sequence models often\nlack generalized chemical knowledge, leading to predictions that are neither\nconsistently accurate nor easily explainable. To address these challenges, we\nintroduce RetroDFM-R, a reasoning-based large language model (LLM) designed\nspecifically for chemical retrosynthesis. Leveraging large-scale reinforcement\nlearning guided by chemically verifiable rewards, RetroDFM-R significantly\nenhances prediction accuracy and explainability. Comprehensive evaluations\ndemonstrate that RetroDFM-R significantly outperforms state-of-the-art methods,\nachieving a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Double-blind\nhuman assessments further validate the chemical plausibility and practical\nutility of RetroDFM-R's predictions. RetroDFM-R also accurately predicts\nmultistep retrosynthetic routes reported in the literature for both real-world\ndrug molecules and perovskite materials. Crucially, the model's explicit\nreasoning process provides human-interpretable insights, thereby enhancing\ntrust and practical value in real-world retrosynthesis applications.",
      "link": "http://arxiv.org/abs/2507.17448v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "On Temporal Guidance and Iterative Refinement in Audio Source Separation",
      "authors": "Tobias Morocutti, Jonathan Greif, Paul Primus, Florian Schmid, Gerhard Widmer",
      "journal": "arXiv",
      "snippet": "Spatial semantic segmentation of sound scenes (S5) involves the accurate\nidentification of active sound classes and the precise separation of their\nsources from complex acoustic mixtures. Conventional systems rely on a\ntwo-stage pipeline - audio tagging followed by label-conditioned source\nseparation - but are often constrained by the absence of fine-grained temporal\ninformation critical for effective separation. In this work, we address this\nlimitation by introducing a novel approach for S5 that enhances the synergy\nbetween the event detection and source separation stages. Our key contributions\nare threefold. First, we fine-tune a pre-trained Transformer to detect active\nsound classes. Second, we utilize a separate instance of this fine-tuned\nTransformer to perform sound event detection (SED), providing the separation\nmodule with detailed, time-varying guidance. Third, we implement an iterative\nrefinement mechanism that progressively enhances separation quality by\nrecursively reusing the separator's output from previous iterations. These\nadvancements lead to significant improvements in both audio tagging and source\nseparation performance, as demonstrated by our system's second-place finish in\nTask 4 of the DCASE Challenge 2025. Our implementation and model checkpoints\nare available in our GitHub repository: https://github.com/theMoro/dcase25task4 .",
      "link": "http://arxiv.org/abs/2507.17297v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for\n  Tumor Flagging and Staging",
      "authors": "Farnaz Khun Jush, Steffen Vogler, Matthias Lenga",
      "journal": "arXiv",
      "snippet": "The increasing volume of medical images poses challenges for radiologists in\nretrieving relevant cases. Content-based image retrieval (CBIR) systems offer\npotential for efficient access to similar cases, yet lack standardized\nevaluation and comprehensive studies. Building on prior studies for tumor\ncharacterization via CBIR, this study advances CBIR research for volumetric\nmedical images through three key contributions: (1) a framework eliminating\nreliance on pre-segmented data and organ-specific datasets, aligning with large\nand unstructured image archiving systems, i.e. PACS in clinical practice; (2)\nintroduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's\ncontextualized late interaction mechanism for 3D medical imaging; (3)\ncomprehensive evaluation across four tumor sites using three feature extractors\nand three database configurations. Our evaluations highlight the significant\nadvantages of C-MIR. We demonstrate the successful adaptation of the late\ninteraction principle to volumetric medical images, enabling effective\ncontext-aware re-ranking. A key finding is C-MIR's ability to effectively\nlocalize the region of interest, eliminating the need for pre-segmentation of\ndatasets and offering a computationally efficient alternative to systems\nrelying on expensive data enrichment steps. C-MIR demonstrates promising\nimprovements in tumor flagging, achieving improved performance, particularly\nfor colon and lung tumors (p<0.05). C-MIR also shows potential for improving\ntumor staging, warranting further exploration of its capabilities. Ultimately,\nour work seeks to bridge the gap between advanced retrieval techniques and\ntheir practical applications in healthcare, paving the way for improved\ndiagnostic processes.",
      "link": "http://arxiv.org/abs/2507.17412v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Efficient Neural Network Verification via Order Leading Exploration of\n  Branch-and-Bound Trees",
      "authors": "Guanqin Zhang, Kota Fukuda, Zhenya Zhang, H. M. N. Dilum Bandara, Shiping Chen, Jianjun Zhao, Yulei Sui",
      "journal": "arXiv",
      "snippet": "The vulnerability of neural networks to adversarial perturbations has\nnecessitated formal verification techniques that can rigorously certify the\nquality of neural networks. As the state-of-the-art, branch and bound (BaB) is\na \"divide-and-conquer\" strategy that applies off-the-shelf verifiers to\nsub-problems for which they perform better. While BaB can identify the\nsub-problems that are necessary to be split, it explores the space of these\nsub-problems in a naive \"first-come-first-serve\" manner, thereby suffering from\nan issue of inefficiency to reach a verification conclusion. To bridge this\ngap, we introduce an order over different sub-problems produced by BaB,\nconcerning with their different likelihoods of containing counterexamples.\nBased on this order, we propose a novel verification framework Oliva that\nexplores the sub-problem space by prioritizing those sub-problems that are more\nlikely to find counterexamples, in order to efficiently reach the conclusion of\nthe verification. Even if no counterexample can be found in any sub-problem, it\nonly changes the order of visiting different sub-problem and so will not lead\nto a performance degradation. Specifically, Oliva has two variants, including\n$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that\nare more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy\ninspired by simulated annealing that gradually shifts from exploration to\nexploitation to locate the globally optimal sub-problems. We experimentally\nevaluate the performance of Oliva on 690 verification problems spanning over 5\nmodels with datasets MNIST and CIFAR10. Compared to the state-of-the-art\napproaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up\nto 80X in CIFAR10.",
      "link": "http://arxiv.org/abs/2507.17453v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large\n  Language Model",
      "authors": "Zhe Xu, Ziyi Liu, Junlin Hou, Jiabo Ma, Cheng Jin, Yihui Wang, Zhixuan Chen, Zhengyu Zhang, Zhengrui Guo, Fengtao Zhou, Yingxue Xu, Xi Wang, Ronald Cheong Kin Chan, Li Liang, Hao Chen",
      "journal": "arXiv",
      "snippet": "Multimodal large language models (MLLMs) have emerged as powerful tools for\ncomputational pathology, offering unprecedented opportunities to integrate\npathological images with language context for comprehensive diagnostic\nanalysis. These models hold particular promise for automating complex tasks\nthat traditionally require expert interpretation of pathologists. However,\ncurrent MLLM approaches in pathology demonstrate significantly constrained\nreasoning capabilities, primarily due to their reliance on expensive\nchain-of-thought annotations. Additionally, existing methods remain limited to\nsimplex application of visual question answering (VQA) at region-of-interest\n(ROI) level, failing to address the full spectrum of diagnostic needs such as\nROI classification, detection, segmentation, whole-slide-image (WSI)\nclassification and VQA in clinical practice. In this study, we present\nSmartPath-R1, a versatile MLLM capable of simultaneously addressing both\nROI-level and WSI-level tasks while demonstrating robust pathological reasoning\ncapability. Our framework combines scale-dependent supervised fine-tuning and\ntask-aware reinforcement fine-tuning, which circumvents the requirement for\nchain-of-thought supervision by leveraging the intrinsic knowledge within MLLM.\nFurthermore, SmartPath-R1 integrates multiscale and multitask analysis through\na mixture-of-experts mechanism, enabling dynamic processing for diverse tasks.\nWe curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI\nsamples for training and evaluation. Extensive experiments across 72 tasks\nvalidate the effectiveness and superiority of the proposed approach. This work\nrepresents a significant step toward developing versatile, reasoning-enhanced\nAI systems for precision pathology.",
      "link": "http://arxiv.org/abs/2507.17303v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration\n  Through Clinical Cognitive Chain Reasoning",
      "authors": "Xinyao Liu, Diping Song",
      "journal": "arXiv",
      "snippet": "Multimodal large language models (MLLMs) demonstrate significant potential in\nthe field of medical diagnosis. However, they face critical challenges in\nspecialized domains such as ophthalmology, particularly the fragmentation of\nannotation granularity and inconsistencies in clinical reasoning logic, which\nhinder precise cross-modal understanding. This paper introduces FundusExpert,\nan ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning\ncapabilities, along with FundusGen, a dataset constructed through the\nintelligent Fundus-Engine system. Fundus-Engine automates localization and\nleverages MLLM-based semantic expansion to integrate global disease\nclassification, local object detection, and fine-grained feature analysis\nwithin a single fundus image. Additionally, by constructing a clinically\naligned cognitive chain, it guides the model to generate interpretable\nreasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,\nachieves the best performance in ophthalmic question-answering tasks,\nsurpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in\nzero-shot report generation tasks, achieving a clinical consistency of 77.0%,\nsignificantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling\nlaw between data quality and model capability ($L \\propto N^{0.068}$),\ndemonstrating that the cognitive alignment annotations in FundusGen enhance\ndata utilization efficiency. By integrating region-level localization with\ndiagnostic reasoning chains, our work develops a scalable, clinically-aligned\nMLLM and explores a pathway toward bridging the visual-language gap in specific\nMLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.",
      "link": "http://arxiv.org/abs/2507.17539v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Learning from Scratch: Structurally-masked Transformer for Next\n  Generation Lib-free Simulation",
      "authors": "Junlang Huang, Hao Chen, Zhong Guan",
      "journal": "arXiv",
      "snippet": "This paper proposes a neural framework for power and timing prediction of\nmulti-stage data path, distinguishing itself from traditional lib-based\nanalytical methods dependent on driver characterization and load\nsimplifications. To the best of our knowledge, this is the first\nlanguage-based, netlist-aware neural network designed explicitly for standard\ncells. Our approach employs two pre-trained neural models of waveform\nprediction and delay estimation that directly infer transient waveforms and\npropagation delays from SPICE netlists, conditioned on critical physical\nparameters such as load capacitance, input slew, and gate size. This method\naccurately captures both intrinsic and coupling-induced delay effects without\nrequiring simplification or interpolation. For multi-stage timing prediction,\nwe implement a recursive propagation strategy where predicted waveforms from\neach stage feed into subsequent stages, cumulatively capturing delays across\nthe logic chain. This approach ensures precise timing alignment and complete\nwaveform visibility throughout complex signal pathways. The waveform prediction\nutilizes a hybrid CNN-Transformer architecture with netlist-aware node-level\nencoding, addressing traditional Transformers' fixed input dimensionality\nconstraints. Additionally, specialized subnetworks separately handle primary\ndelay estimation and crosstalk correction. Experimental results demonstrate\nSPICE-level accuracy, consistently achieving RMSE below 0.0098 across diverse\nindustrial circuits. The proposed framework provides a scalable, structurally\nadaptable neural alternative to conventional power and timing engines,\ndemonstrating high fidelity to physical circuit behaviors.",
      "link": "http://arxiv.org/abs/2507.17396v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with\n  Sequential Mixture of Experts for Multi-View Mammography",
      "authors": "Farnoush Bayatmakou, Reza Taleei, Nicole Simone, Arash Mohammadi",
      "journal": "arXiv",
      "snippet": "Breast cancer (BC) remains one of the leading causes of cancer-related\nmortality among women, despite recent advances in Computer-Aided Diagnosis\n(CAD) systems. Accurate and efficient interpretation of multi-view mammograms\nis essential for early detection, driving a surge of interest in Artificial\nIntelligence (AI)-powered CAD models. While state-of-the-art multi-view\nmammogram classification models are largely based on Transformer architectures,\ntheir computational complexity scales quadratically with the number of image\npatches, highlighting the need for more efficient alternatives. To address this\nchallenge, we propose Mammo-Mamba, a novel framework that integrates Selective\nState-Space Models (SSMs), transformer-based attention, and expert-driven\nfeature refinement into a unified architecture. Mammo-Mamba extends the\nMambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE)\nmechanism through its customized SecMamba block. The SecMamba is a modified\nMambaVision block that enhances representation learning in high-resolution\nmammographic images by enabling content-adaptive feature refinement. These\nblocks are integrated into the deeper stages of MambaVision, allowing the model\nto progressively adjust feature emphasis through dynamic expert gating,\neffectively mitigating the limitations of traditional Transformer models.\nEvaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superior\nclassification performance across all key metrics while maintaining\ncomputational efficiency.",
      "link": "http://arxiv.org/abs/2507.17662v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "To Trust or Not to Trust: On Calibration in ML-based Resource Allocation\n  for Wireless Networks",
      "authors": "Rashika Raina, Nidhi Simmons, David E. Simmons, Michel Daoud Yacoub, Trung Q. Duong",
      "journal": "arXiv",
      "snippet": "In next-generation communications and networks, machine learning (ML) models\nare expected to deliver not only accurate predictions but also well-calibrated\nconfidence scores that reflect the true likelihood of correct decisions. This\npaper studies the calibration performance of an ML-based outage predictor\nwithin a single-user, multi-resource allocation framework. We first establish\nkey theoretical properties of this system's outage probability (OP) under\nperfect calibration. Importantly, we show that as the number of resources\ngrows, the OP of a perfectly calibrated predictor approaches the expected\noutput conditioned on it being below the classification threshold. In contrast,\nwhen only one resource is available, the system's OP equals the model's overall\nexpected output. We then derive the OP conditions for a perfectly calibrated\npredictor. These findings guide the choice of the classification threshold to\nachieve a desired OP, helping system designers meet specific reliability\nrequirements. We also demonstrate that post-processing calibration cannot\nimprove the system's minimum achievable OP, as it does not introduce new\ninformation about future channel states. Additionally, we show that\nwell-calibrated models are part of a broader class of predictors that\nnecessarily improve OP. In particular, we establish a monotonicity condition\nthat the accuracy-confidence function must satisfy for such improvement to\noccur. To demonstrate these theoretical properties, we conduct a rigorous\nsimulation-based analysis using post-processing calibration techniques: Platt\nscaling and isotonic regression. As part of this framework, the predictor is\ntrained using an outage loss function specifically designed for this system.\nFurthermore, this analysis is performed on Rayleigh fading channels with\ntemporal correlation captured by Clarke's 2D model, which accounts for receiver\nmobility.",
      "link": "http://arxiv.org/abs/2507.17494v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks",
      "authors": "Ilias Chatzistefanidis, Navid Nikaein",
      "journal": "arXiv",
      "snippet": "Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance.",
      "link": "http://arxiv.org/abs/2507.17695v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Agent Identity Evals: Measuring Agentic Identity",
      "authors": "Elija Perrier, Michael Timothy Bennett",
      "journal": "arXiv",
      "snippet": "Central to agentic capability and trustworthiness of language model agents\n(LMAs) is the extent they maintain stable, reliable, identity over time.\nHowever, LMAs inherit pathologies from large language models (LLMs)\n(statelessness, stochasticity, sensitivity to prompts and\nlinguistically-intermediation) which can undermine their identifiability,\ncontinuity, persistence and consistency. This attrition of identity can erode\ntheir reliability, trustworthiness and utility by interfering with their\nagentic capabilities such as reasoning, planning and action. To address these\nchallenges, we introduce \\textit{agent identity evals} (AIE), a rigorous,\nstatistically-driven, empirical framework for measuring the degree to which an\nLMA system exhibit and maintain their agentic identity over time, including\ntheir capabilities, properties and ability to recover from state perturbations.\nAIE comprises a set of novel metrics which can integrate with other measures of\nperformance, capability and agentic robustness to assist in the design of\noptimal LMA infrastructure and scaffolding such as memory and tools. We set out\nformal definitions and methods that can be applied at each stage of the LMA\nlife-cycle, and worked examples of how to apply them.",
      "link": "http://arxiv.org/abs/2507.17257v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains",
      "authors": "Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, Sean Hendryx",
      "journal": "arXiv",
      "snippet": "Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world\ntasks often requires balancing objective and subjective evaluation criteria.\nHowever, many such tasks lack a single, unambiguous ground truth-making it\ndifficult to define reliable reward signals for post-training language models.\nWhile traditional preference-based methods offer a workaround, they rely on\nopaque reward functions that are difficult to interpret and prone to spurious\ncorrelations. We introduce $\\textbf{Rubrics as Rewards}$ (RaR), a framework\nthat uses structured, checklist-style rubrics as interpretable reward signals\nfor on-policy training with GRPO. Our best RaR method yields up to a $28\\%$\nrelative improvement on HealthBench-1k compared to simple Likert-based\napproaches, while matching or surpassing the performance of reward signals\nderived from expert-written references. By treating rubrics as structured\nreward signals, we show that RaR enables smaller-scale judge models to better\nalign with human preferences and sustain robust performance across model\nscales.",
      "link": "http://arxiv.org/abs/2507.17746v1",
      "published_date": "July 23, 2025",
      "published_date_sort": "2025-07-23",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data",
      "authors": "A Lapin, I Hromov, S Chumakov, M Mitrovic… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "AutoML has advanced in handling complex tasks using the integration of LLMs, yet its efficiency remains limited by dependence on specific underlying tools. In this paper, we …",
      "link": "https://arxiv.org/abs/2507.13413",
      "published_date": "July 2025",
      "published_date_sort": "2025-07-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors",
      "authors": "Y Chen, P Piȩkos, M Ostaszewski, F Laakom… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior …",
      "link": "https://arxiv.org/abs/2507.15550",
      "published_date": "July 2025",
      "published_date_sort": "2025-07-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science",
      "authors": "A Luo, X Xian, J Du, F Tian, G Wang, M Zhong… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as …",
      "link": "https://arxiv.org/abs/2506.13992",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research",
      "authors": "S Yan, R Li, Z Luo, Z Wang, D Li, L Jing, K He… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of …",
      "link": "https://arxiv.org/abs/2506.17335",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
      "authors": "B Zhao, D Magka, M Jiang, X Li, R Raileanu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce …",
      "link": "https://arxiv.org/abs/2506.22419",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AI Scientists Fail Without Strong Implementation Capability",
      "authors": "M Zhu, Q Xie, Y Weng, J Wu, Z Lin, L Yang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the …",
      "link": "https://arxiv.org/abs/2506.01372",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement",
      "authors": "J Nam, J Yoon, J Chen, J Shin, SÖ Arık… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Agents based on large language models (LLMs) for machine learning engineering (MLE) can automatically implement ML models via code generation. However, existing approaches …",
      "link": "https://arxiv.org/abs/2506.15692",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Towards Community-Driven Agents for Machine Learning Engineering",
      "authors": "S Li, W Sun, S Li, A Talwalkar, Y Yang - arXiv preprint arXiv:2506.20640, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given …",
      "link": "https://arxiv.org/abs/2506.20640",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis",
      "authors": "H Li, H Liu, T Zhu, T Guo, Z Zheng, X Deng… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with …",
      "link": "https://arxiv.org/abs/2505.18223",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Mlzero: A multi-agent system for end-to-end machine learning automation",
      "authors": "H Fang, B Han, N Erickson, X Zhang, S Zhou… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly …",
      "link": "https://arxiv.org/abs/2505.13941",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving",
      "authors": "H Wang, Z Ni, S Zhang, S Lu, S Hu, Z He, C Hu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world …",
      "link": "https://arxiv.org/abs/2505.21577",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Mle-dojo: Interactive environments for empowering llm agents in machine learning engineering",
      "authors": "R Qiang, Y Zhuang, Y Li, R Zhang, C Li… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative …",
      "link": "https://arxiv.org/abs/2505.07782",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "From automation to autonomy: A survey on large language models in scientific discovery",
      "authors": "T Zheng, Z Deng, HT Tsang, W Wang, J Bai… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and …",
      "link": "https://arxiv.org/abs/2505.13259",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 3,
      "institutions": "Unknown"
    },
    {
      "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents",
      "authors": "Y Cai, X Li, M Goswami, M Wiliński, G Welter… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating Artificial Intelligence (AI) agents on time series machine learning engineering challenges. Existing …",
      "link": "https://arxiv.org/abs/2505.13291",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
      "authors": "S Li, T Marwah, J Shen, W Sun, A Risteski… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert …",
      "link": "https://arxiv.org/abs/2505.08783",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Co-bench: Benchmarking language model agents in algorithm search for combinatorial optimization",
      "authors": "W Sun, S Feng, S Li, Y Yang - arXiv preprint arXiv:2504.04310, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial …",
      "link": "https://arxiv.org/abs/2504.04310",
      "published_date": "April 2025",
      "published_date_sort": "2025-04-01",
      "citations": 6,
      "institutions": "Unknown"
    },
    {
      "title": "Measuring ai ability to complete long tasks",
      "authors": "T Kwa, B West, J Becker, A Deng, K Garcia… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human …",
      "link": "https://arxiv.org/abs/2503.14499",
      "published_date": "March 2025",
      "published_date_sort": "2025-03-01",
      "citations": 21,
      "institutions": "Unknown"
    },
    {
      "title": "PiML: Automated Machine Learning Workflow Optimization using LLM Agents",
      "authors": "A Chopde, F Pettiwala, S Kirubananth, SK Botla… - AutoML 2025 Methods … - openreview.net",
      "journal": "Google Scholar Result",
      "snippet": "In this paper, we introduce PiML-Persistent Iterative Machine Learning agentic framework, a novel automated pipeline specifically designed for solving real-world machine learning (ML) …",
      "link": "https://openreview.net/forum?id=Nw1qBpsjZz",
      "published_date": "2025",
      "published_date_sort": "2025-01-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Towards Pragmatic Time Series Intelligence",
      "authors": "M Goswami - 2025 - search.proquest.com",
      "journal": "Google Scholar Result",
      "snippet": "This thesis aims to democratize time series intelligence by making advanced modeling capabilities accessible to users without specialized machine learning knowledge. We …",
      "link": "https://search.proquest.com/openview/24ff18df8583af7a835b667b59b216b9/1?pq-origsite=gscholar&cbl=18750&diss=y",
      "published_date": "2025",
      "published_date_sort": "2025-01-01",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "AutoCluster: Un agent pour le clustering basé sur les grands modèles de langue",
      "authors": "E Versmée, Y Remil, M Kaytoue, J Velcin - coria-taln-2025.lis-lab.fr",
      "journal": "Google Scholar Result",
      "snippet": "Cette recherche présente AutoCluster, un agent basé sur les grands modèles de langue pour des tâches de classification non supervisée. Nous concevons trois agents dont deux …",
      "link": "https://coria-taln-2025.lis-lab.fr/wp-content/uploads/2025/06/CORIA-TALN_2025_paper_124.pdf",
      "published_date": "2025",
      "published_date_sort": "2025-01-01",
      "citations": 0,
      "institutions": "Unknown"
    }
  ]
}