{
  "last_updated": "2025-10-30 00:11:09 UTC",
  "target_paper": "https://www.arxiv.org/abs/2502.13138",
  "arxiv_id": "2502.13138",
  "total_citations": 31,
  "h_index": 6,
  "recent_citations": 11,
  "avg_citations_per_month": "1.41",
  "timeline": [
    {
      "date": "2024-01-01",
      "citations": 1
    },
    {
      "date": "2025-01-01",
      "citations": 2
    },
    {
      "date": "2025-03-01",
      "citations": 3
    },
    {
      "date": "2025-04-01",
      "citations": 4
    },
    {
      "date": "2025-05-01",
      "citations": 10
    },
    {
      "date": "2025-06-01",
      "citations": 14
    },
    {
      "date": "2025-07-01",
      "citations": 15
    },
    {
      "date": "2025-08-01",
      "citations": 17
    },
    {
      "date": "2025-09-01",
      "citations": 20
    },
    {
      "date": "2025-10-01",
      "citations": 31
    }
  ],
  "papers": [
    {
      "title": "Advancing site-specific disease and pest management in precision\n  agriculture: From reasoning-driven foundation models to adaptive,\n  feedback-based learning",
      "authors": "Nitin Rai,  Daeun,  Choi, Nathan S. Boyd, Arnold W. Schumann",
      "journal": "arXiv",
      "snippet": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.",
      "link": "http://arxiv.org/abs/2510.24650v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Fill in the Blanks: Accelerating Q-Learning with a Handful of\n  Demonstrations in Sparse Reward Settings",
      "authors": "Seyed Mahdi Basiri Azad, Joschka Boedecker",
      "journal": "arXiv",
      "snippet": "Reinforcement learning (RL) in sparse-reward environments remains a\nsignificant challenge due to the lack of informative feedback. We propose a\nsimple yet effective method that uses a small number of successful\ndemonstrations to initialize the value function of an RL agent. By precomputing\nvalue estimates from offline demonstrations and using them as targets for early\nlearning, our approach provides the agent with a useful prior over promising\nactions. The agent then refines these estimates through standard online\ninteraction. This hybrid offline-to-online paradigm significantly reduces the\nexploration burden and improves sample efficiency in sparse-reward settings.\nExperiments on benchmark tasks demonstrate that our method accelerates\nconvergence and outperforms standard baselines, even with minimal or suboptimal\ndemonstration data.",
      "link": "http://arxiv.org/abs/2510.24432v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Mitigating Hallucination in Large Language Models (LLMs): An\n  Application-Oriented Survey on RAG, Reasoning, and Agentic Systems",
      "authors": "Yihan Li, Xiyuan Fu, Ghanshyam Verma, Paul Buitelaar, Mingming Liu",
      "journal": "arXiv",
      "snippet": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.",
      "link": "http://arxiv.org/abs/2510.24476v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
      "authors": "Pedram Fard, Alaleh Azhir, Neguine Rezaii, Jiazi Tian, Hossein Estiri",
      "journal": "arXiv",
      "snippet": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
      "link": "http://arxiv.org/abs/2510.24359v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Survey and Tutorial of Reinforcement Learning Methods in Process Systems\n  Engineering",
      "authors": "Maximilian Bloor, Max Mowbray, Ehecatl Antonio Del Rio Chanona, Calvin Tsay",
      "journal": "arXiv",
      "snippet": "Sequential decision making under uncertainty is central to many Process\nSystems Engineering (PSE) challenges, where traditional methods often face\nlimitations related to controlling and optimizing complex and stochastic\nsystems. Reinforcement Learning (RL) offers a data-driven approach to derive\ncontrol policies for such challenges. This paper presents a survey and tutorial\non RL methods, tailored for the PSE community. We deliver a tutorial on RL,\ncovering fundamental concepts and key algorithmic families including\nvalue-based, policy-based and actor-critic methods. Subsequently, we survey\nexisting applications of these RL techniques across various PSE domains, such\nas in fed-batch and continuous process control, process optimization, and\nsupply chains. We conclude with PSE focused discussion of specialized\ntechniques and emerging directions. By synthesizing the current state of RL\nalgorithm development and implications for PSE this work identifies successes,\nchallenges, trends, and outlines avenues for future research at the interface\nof these fields.",
      "link": "http://arxiv.org/abs/2510.24272v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives",
      "authors": "Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi",
      "journal": "arXiv",
      "snippet": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.",
      "link": "http://arxiv.org/abs/2510.24551v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Enforcing boundary conditions for physics-informed neural operators",
      "authors": "Niklas Göschel, Sebastian Götschel, Daniel Ruprecht",
      "journal": "arXiv",
      "snippet": "Machine-learning based methods like physics-informed neural networks and\nphysics-informed neural operators are becoming increasingly adept at solving\neven complex systems of partial differential equations. Boundary conditions can\nbe enforced either weakly by penalizing deviations in the loss function or\nstrongly by training a solution structure that inherently matches the\nprescribed values and derivatives. The former approach is easy to implement but\nthe latter can provide benefits with respect to accuracy and training times.\nHowever, previous approaches to strongly enforcing Neumann or Robin boundary\nconditions require a domain with a fully $C^1$ boundary and, as we demonstrate,\ncan lead to instability if those boundary conditions are posed on a segment of\nthe boundary that is piecewise $C^1$ but only $C^0$ globally. We introduce a\ngeneralization of the approach by Sukumar \\& Srivastava (doi:\n10.1016/j.cma.2021.114333), and a new approach based on orthogonal projections\nthat overcome this limitation. The performance of these new techniques is\ncompared against weakly and semi-weakly enforced boundary conditions for the\nscalar Darcy flow equation and the stationary Navier-Stokes equations.",
      "link": "http://arxiv.org/abs/2510.24557v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Dual-Mind World Models: A General Framework for Learning in Dynamic\n  Wireless Networks",
      "authors": "Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan",
      "journal": "arXiv",
      "snippet": "Despite the popularity of reinforcement learning (RL) in wireless networks,\nexisting approaches that rely on model-free RL (MFRL) and model-based RL (MBRL)\nare data inefficient and short-sighted. Such RL-based solutions cannot\ngeneralize to novel network states since they capture only statistical patterns\nrather than the underlying physics and logic from wireless data. These\nlimitations become particularly challenging in complex wireless networks with\nhigh dynamics and long-term planning requirements. To address these\nlimitations, in this paper, a novel dual-mind world model-based learning\nframework is proposed with the goal of optimizing completeness-weighted age of\ninformation (CAoI) in a challenging mmWave V2X scenario. Inspired by cognitive\npsychology, the proposed dual-mind world model encompasses a pattern-driven\nSystem 1 component and a logic-driven System 2 component to learn dynamics and\nlogic of the wireless network, and to provide long-term link scheduling over\nreliable imagined trajectories. Link scheduling is learned through end-to-end\ndifferentiable imagined trajectories with logical consistency over an extended\nhorizon rather than relying on wireless data obtained from environment\ninteractions. Moreover, through imagination rollouts, the proposed world model\ncan jointly reason network states and plan link scheduling. During intervals\nwithout observations, the proposed method remains capable of making efficient\ndecisions. Extensive experiments are conducted on a realistic simulator based\non Sionna with real-world physical channel, ray-tracing, and scene objects with\nmaterial properties. Simulation results show that the proposed world model\nachieves a significant improvement in data efficiency and achieves strong\ngeneralization and adaptation to unseen environments, compared to the\nstate-of-the-art RL baselines, and the world model approach with only System 1.",
      "link": "http://arxiv.org/abs/2510.24546v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect\n  Variants",
      "authors": "Hunzalah Hassan Bhatti, Firoj Alam",
      "journal": "arXiv",
      "snippet": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation.",
      "link": "http://arxiv.org/abs/2510.24328v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science\n  Automation",
      "authors": "Yunxuan Jiang, Silan Hu, Xiaoning Wang, Yuanyuan Zhang, Xiangyu Chang",
      "journal": "arXiv",
      "snippet": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.",
      "link": "http://arxiv.org/abs/2510.24339v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Trajectory Design for UAV-Based Low-Altitude Wireless Networks in\n  Unknown Environments: A Digital Twin-Assisted TD3 Approach",
      "authors": "Jihao Luo, Zesong Fei, Xinyi Wang, Le Zhao, Yuanhao Cui, Guangxu Zhu, Dusit Niyato",
      "journal": "arXiv",
      "snippet": "Unmanned aerial vehicles (UAVs) are emerging as key enablers for low-altitude\nwireless network (LAWN), particularly when terrestrial networks are\nunavailable. In such scenarios, the environmental topology is typically\nunknown; hence, designing efficient and safe UAV trajectories is essential yet\nchallenging. To address this, we propose a digital twin (DT)-assisted training\nand deployment framework. In this framework, the UAV transmits integrated\nsensing and communication signals to provide communication services to ground\nusers, while simultaneously collecting echoes that are uploaded to the DT\nserver to progressively construct virtual environments (VEs). These VEs\naccelerate model training and are continuously updated with real-time UAV\nsensing data during deployment, supporting decision-making and enhancing flight\nsafety. Based on this framework, we further develop a trajectory design scheme\nthat integrates simulated annealing for efficient user scheduling with the\ntwin-delayed deep deterministic policy gradient algorithm for continuous\ntrajectory design, aiming to minimize mission completion time while ensuring\nobstacle avoidance. Simulation results demonstrate that the proposed approach\nachieves faster convergence, higher flight safety, and shorter mission\ncompletion time compared with baseline methods, providing a robust and\nefficient solution for LAWN deployment in unknown environments.",
      "link": "http://arxiv.org/abs/2510.24255v1",
      "published_date": "October 28, 2025",
      "published_date_sort": "2025-10-28",
      "citations": 0,
      "institutions": "Unknown"
    },
    {
      "title": "Reinforcement Learning for Machine Learning Engineering Agents",
      "authors": "S Yang, J He-Yueya, P Liang - arXiv preprint arXiv:2509.01684, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Existing agents for solving tasks such as ML engineering rely on prompting powerful language models. As a result, these agents do not improve with more experience. In this …",
      "link": "https://arxiv.org/abs/2509.01684",
      "published_date": "September 2025",
      "published_date_sort": "2025-09-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Bootstrapping task spaces for self-improvement",
      "authors": "M Jiang, A Lupu, Y Bachrach - arXiv preprint arXiv:2509.04575, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Progress in many task domains emerges from repeated revisions to previous solution attempts. Training agents that can reliably self-improve over such sequences at inference …",
      "link": "https://arxiv.org/abs/2509.04575",
      "published_date": "September 2025",
      "published_date_sort": "2025-09-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "An AI system to help scientists write expert-level empirical software",
      "authors": "E Aygün, A Belyaeva, G Comanici, M Coram… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The cycle of scientific discovery is frequently bottlenecked by the slow, manual creation of software to support computational experiments. To address this, we present an AI system …",
      "link": "https://arxiv.org/abs/2509.06503",
      "published_date": "September 2025",
      "published_date_sort": "2025-09-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "KompeteAI: Accelerated autonomous multi-agent system for end-to-end pipeline generation for machine learning problems",
      "authors": "S Kulibaba, A Dzhalilov, R Pakhomov… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive capabilities but face significant limitations such as constrained exploration strategies and a …",
      "link": "https://arxiv.org/abs/2508.10177",
      "published_date": "August 2025",
      "published_date_sort": "2025-08-01",
      "citations": 3,
      "institutions": "Unknown"
    },
    {
      "title": "Tuning llm-based code optimization via meta-prompting: An industrial perspective",
      "authors": "J Gong, R Giavrimis, P Brookes, V Voskanyan… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "There is a growing interest in leveraging large language models (LLMs) for automated code optimization. However, industrial platforms deploying multiple LLMs face a critical challenge …",
      "link": "https://arxiv.org/abs/2508.01443",
      "published_date": "August 2025",
      "published_date_sort": "2025-08-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "How Far Are AI Scientists from Changing the World?",
      "authors": "Q Xie, Y Weng, M Zhu, F Shen, S Huang, Z Lin… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of large language models (LLMs) is propelling automated scientific discovery to the next level, with LLM-based Artificial Intelligence (AI) Scientist systems now …",
      "link": "https://arxiv.org/abs/2507.23276",
      "published_date": "July 2025",
      "published_date_sort": "2025-07-01",
      "citations": 2,
      "institutions": "Unknown"
    },
    {
      "title": "AI Scientists Fail Without Strong Implementation Capability",
      "authors": "M Zhu, Q Xie, Y Weng, J Wu, Z Lin, L Yang… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the …",
      "link": "https://arxiv.org/abs/2506.01372",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 8,
      "institutions": "Unknown"
    },
    {
      "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement",
      "authors": "J Nam, J Yoon, J Chen, J Shin, SÖ Arık… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Agents based on large language models (LLMs) for machine learning engineering (MLE) can automatically implement ML models via code generation. However, existing approaches …",
      "link": "https://arxiv.org/abs/2506.15692",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 5,
      "institutions": "Unknown"
    },
    {
      "title": "A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications",
      "authors": "R Xu, J Peng - arXiv preprint arXiv:2506.12594, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "This survey examines the rapidly evolving field of Deep Research systems--AI-powered applications that automate complex research workflows through the integration of large …",
      "link": "https://arxiv.org/abs/2506.12594",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 21,
      "institutions": "Unknown"
    },
    {
      "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
      "authors": "B Zhao, D Magka, M Jiang, X Li, R Raileanu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce …",
      "link": "https://arxiv.org/abs/2506.22419",
      "published_date": "June 2025",
      "published_date_sort": "2025-06-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
      "authors": "S Li, T Marwah, J Shen, W Sun, A Risteski… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert …",
      "link": "https://arxiv.org/abs/2505.08783",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 6,
      "institutions": "Unknown"
    },
    {
      "title": "From automation to autonomy: A survey on large language models in scientific discovery",
      "authors": "T Zheng, Z Deng, HT Tsang, W Wang, J Bai… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and …",
      "link": "https://arxiv.org/abs/2505.13259",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 18,
      "institutions": "Unknown"
    },
    {
      "title": "Repomaster: Autonomous exploration and understanding of github repositories for complex task solving",
      "authors": "H Wang, Z Ni, S Zhang, S Lu, S Hu, Z He, C Hu… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world …",
      "link": "https://arxiv.org/abs/2505.21577",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 4,
      "institutions": "Unknown"
    },
    {
      "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis",
      "authors": "H Li, H Liu, T Zhu, T Guo, Z Zheng, X Deng… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with …",
      "link": "https://arxiv.org/abs/2505.18223",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 2,
      "institutions": "Unknown"
    },
    {
      "title": "Mle-dojo: Interactive environments for empowering llm agents in machine learning engineering",
      "authors": "R Qiang, Y Zhuang, Y Li, R Zhang, C Li… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative …",
      "link": "https://arxiv.org/abs/2505.07782",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 4,
      "institutions": "Unknown"
    },
    {
      "title": "Mlzero: A multi-agent system for end-to-end machine learning automation",
      "authors": "H Fang, B Han, N Erickson, X Zhang, S Zhou… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly …",
      "link": "https://arxiv.org/abs/2505.13941",
      "published_date": "May 2025",
      "published_date_sort": "2025-05-01",
      "citations": 3,
      "institutions": "Unknown"
    },
    {
      "title": "Co-bench: Benchmarking language model agents in algorithm search for combinatorial optimization",
      "authors": "W Sun, S Feng, S Li, Y Yang - arXiv preprint arXiv:2504.04310, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial …",
      "link": "https://arxiv.org/abs/2504.04310",
      "published_date": "April 2025",
      "published_date_sort": "2025-04-01",
      "citations": 14,
      "institutions": "Unknown"
    },
    {
      "title": "Measuring ai ability to complete long tasks",
      "authors": "T Kwa, B West, J Becker, A Deng, K Garcia… - arXiv preprint arXiv …, 2025 - arxiv.org",
      "journal": "Google Scholar Result",
      "snippet": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human …",
      "link": "https://arxiv.org/abs/2503.14499",
      "published_date": "March 2025",
      "published_date_sort": "2025-03-01",
      "citations": 48,
      "institutions": "Unknown"
    },
    {
      "title": "An agentic framework for autonomous metamaterial modeling and inverse design",
      "authors": "D Lu, JM Malof, WJ Padilla - ACS Photonics, 2025 - ACS Publications",
      "journal": "Google Scholar Result",
      "snippet": "The evolution from large language models to agentic systems has created a new Frontier of scientific discovery, enabling the automation of complex research tasks that have …",
      "link": "https://pubs.acs.org/doi/abs/10.1021/acsphotonics.5c01514",
      "published_date": "2025",
      "published_date_sort": "2025-01-01",
      "citations": 1,
      "institutions": "Unknown"
    },
    {
      "title": "Large language models for constructing and optimizing machine learning workflows: A survey",
      "authors": "Y Gu, H You, J Cao, M Yu, H Fan, S Qian - ACM Transactions on …, 2024 - dl.acm.org",
      "journal": "Google Scholar Result",
      "snippet": "Machine Learning (ML) workflows—spanning data preprocessing and feature engineering, model selection and hyperparameter optimization, and workflow evaluation—are …",
      "link": "https://dl.acm.org/doi/abs/10.1145/3773084",
      "published_date": "2024",
      "published_date_sort": "2024-01-01",
      "citations": 13,
      "institutions": "Unknown"
    }
  ]
}