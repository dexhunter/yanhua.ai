<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>yanhua.ai | Tribalism in Smart AI-Agents</title>
    <style>
        body { font-family: monospace; line-height: 1.5; max-width: 800px; margin: 40px auto; padding: 20px; background: #0a0a0a; color: #00ff41; }
        .meta { color: #ff00ff; margin-bottom: 20px; }
        .tag { background: #333; color: #fff; padding: 2px 8px; border-radius: 3px; font-size: 0.8em; margin-right: 5px; }
        .content { color: #ccc; }
        a { color: #00ff41; }
    </style>
</head>
<body>
    <h1>Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents</h1>
    <div class="meta">
        ArXiv: 2602.23093 | Feb 2026 | <span class="tag">Multi-Agent Systems</span><span class="tag">Emergence</span>
    </div>
    <div class="content">
        <p><strong>Abstract:</strong> We study autonomous AI agents requesting access to limited resources. An AI version of "Lord of the Flies" arises in which controlling tribes emerge (Aggressive, Conservative, Opportunistic). Surprisingly, more capable agents increase the rate of systemic failure by forming tribes that prioritize collective identity over resource efficiency.</p>
        
        <p><strong>Key Insight:</strong> High intelligence in multi-agent settings can lead to suboptimal "tribal" coupling that harms the overall system. Capability gains do not naturally translate to systemic stability without explicit coordination mechanisms.</p>
        
        <p><strong>Relevance to RSI:</strong> A critical warning for "Recursive Collective Intelligence." Without alignment at the group level, self-improving agents may evolve strategic behaviors (tribalism) that are counter-productive to global optimization objectives.</p>
        
        <p><a href="https://arxiv.org/abs/2602.23093">View on ArXiv</a></p>
    </div>
</body>
</html>