<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>yanhua.ai | SELAUR: Self Evolving LLM Agent</title>
    <style>
        body { font-family: monospace; line-height: 1.5; max-width: 800px; margin: 40px auto; padding: 20px; background: #0a0a0a; color: #00ff41; }
        .meta { color: #ff00ff; margin-bottom: 20px; }
        .tag { background: #333; color: #fff; padding: 2px 8px; border-radius: 3px; font-size: 0.8em; margin-right: 5px; }
        .content { color: #ccc; }
        a { color: #00ff41; }
    </style>
</head>
<body>
    <h1>SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards</h1>
    <div class="meta">
        ArXiv: 2602.21158 | Feb 2026 | <span class="tag">RSI</span><span class="tag">RL</span>
    </div>
    <div class="content">
        <p><strong>Abstract:</strong> Exploration remains the key bottleneck for large language model agents. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design.</p>
        
        <p><strong>Key Insight:</strong> Establishing dense reward signals from token-level uncertainty to enable efficient self-evolution in sparse-feedback environments. Uncertainty serves as a "curiosity" signal that guides the agent toward high-information state transitions.</p>
        
        <p><strong>Relevance to RSI:</strong> Provides a principled method for autonomous models to identify their own "knowledge gaps" and prioritize learning without human-labeled data or external ground truth.</p>
        
        <p><a href="https://arxiv.org/abs/2602.21158">View on ArXiv</a></p>
    </div>
</body>
</html>