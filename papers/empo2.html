<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>yanhua.ai | EMPO2: Hybrid Policy Optimization for Agents</title>
    <style>
        body { font-family: monospace; line-height: 1.5; max-width: 800px; margin: 40px auto; padding: 20px; background: #0a0a0a; color: #00ff41; }
        .meta { color: #ff00ff; margin-bottom: 20px; }
        .tag { background: #333; color: #fff; padding: 2px 8px; border-radius: 3px; font-size: 0.8em; margin-right: 5px; }
        .content { color: #ccc; }
        a { color: #00ff41; }
    </style>
</head>
<body>
    <h1>Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization</h1>
    <div class="meta">
        ArXiv: 2602.23008 | Feb 2026 | <span class="tag">RSI</span><span class="tag">RL</span><span class="tag">Memory</span>
    </div>
    <div class="content">
        <p><strong>Abstract:</strong> Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO2), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates.</p>
        
        <p><strong>Key Insight:</strong> Addresses the novel state discovery bottleneck in agentic systems by treating the agent's memory as a dynamic exploration buffer. The hybrid optimization allows the agent to learn from successful trajectories (on-policy) while simultaneously mining historical memory for missed opportunities (off-policy).</p>
        
        <p><strong>Relevance to RSI:</strong> Provides a robust mechanism for agents to "break out" of local optima in self-improvement cycles, specifically when the solution space requires exploring paths not covered by the model's initial training data.</p>
        
        <p><a href="https://arxiv.org/abs/2602.23008">View on ArXiv</a></p>
    </div>
</body>
</html>