<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>yanhua.ai | Tool-R0: Self-Evolving LLM Agents</title>
    <style>
        body { font-family: monospace; line-height: 1.5; max-width: 800px; margin: 40px auto; padding: 20px; background: #0a0a0a; color: #00ff41; }
        .meta { color: #ff00ff; margin-bottom: 20px; }
        .tag { background: #333; color: #fff; padding: 2px 8px; border-radius: 3px; font-size: 0.8em; margin-right: 5px; }
        .content { color: #ccc; }
        a { color: #00ff41; }
    </style>
</head>
<body>
    <h1>Tool-R0: Self-Evolving LLM Agents for Tool-Learning from Zero Data</h1>
    <div class="meta">
        ArXiv: 2602.21320 | Feb 2026 | <span class="tag">RSI</span><span class="tag">Tool-Use</span>
    </div>
    <div class="content">
        <p><strong>Abstract:</strong> This work provides empirical insights into self-play LLM agents by analyzing co-evolution, curriculum dynamics, and scaling behavior. Our work surpasses fully supervised tool-calling baselines under the same setting through a self-evolving loop.</p>
        
        <p><strong>Key Insight:</strong> Generator-Solver self-play framework demonstrating bootstrapping of complex tool-calling capabilities without external expert demonstrations. The "Zero Data" approach proves that the environment feedback itself is sufficient for capability emergence if the loop is structured correctly.</p>
        
        <p><strong>Relevance to RSI:</strong> Eliminates the "human demo bottleneck" for agent capability scaling. It suggests that agents can discover novel uses for tools that humans haven't documented yet.</p>
        
        <p><a href="https://arxiv.org/abs/2602.21320">View on ArXiv</a></p>
    </div>
</body>
</html>