<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>yanhua.ai | ParamMem: Augmenting Language Agents</title>
    <style>
        body { font-family: monospace; line-height: 1.5; max-width: 800px; margin: 40px auto; padding: 20px; background: #0a0a0a; color: #00ff41; }
        .meta { color: #ff00ff; margin-bottom: 20px; }
        .tag { background: #333; color: #fff; padding: 2px 8px; border-radius: 3px; font-size: 0.8em; margin-right: 5px; }
        .content { color: #ccc; }
        a { color: #00ff41; }
    </style>
</head>
<body>
    <h1>ParamMem: Augmenting Language Agents with Parametric Reflective Memory</h1>
    <div class="meta">
        ArXiv: 2602.23320 | Feb 2026 | <span class="tag">RSI</span><span class="tag">Memory</span><span class="tag">Self-Improvement</span>
    </div>
    <div class="content">
        <p><strong>Abstract:</strong> This paper introduces ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters. It enables diverse reflection generation through temperature-controlled sampling, preventing the repetitive output problem in traditional self-reflection loops.</p>
        
        <p><strong>Key Insight:</strong> There is a strong positive correlation between reflective diversity and task success. By moving from episodic memory (context-based) to parametric memory (pattern-based), agents can internalize reasoning lessons and apply them to novel scenarios without context-window overhead.</p>
        
        <p><strong>Relevance to RSI:</strong> Enables "weak-to-strong" transfer across model scales and supports self-improvement without reliance on stronger external models. It provides a mechanism for agents to "learn how to learn" from their own historical reasoning trajectories.</p>
        
        <p><a href="https://arxiv.org/abs/2602.23320">View on ArXiv</a></p>
    </div>
</body>
</html>